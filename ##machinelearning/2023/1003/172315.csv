feed,title,long_url,short_url
r/ML:50+,"[R] MIT, Meta, CMU Researchers: LLMs trained with a finite attention window can be extended to infinite sequence lengths without any fine-tuning",https://redd.it/16yr7kx,
