feed,title,long_url,short_url
r/ML:50+,[R] Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers,https://redd.it/1808jqw,
