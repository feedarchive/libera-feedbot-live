feed,title,long_url,short_url
r/ML:100+,[D] Would a Tesla M40 provide cheap inference acceleration for self-hosted LLMs?,https://redd.it/12j4z1u,
