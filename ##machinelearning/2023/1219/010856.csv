feed,title,long_url,short_url
PwC:Trending,/apple/ Stabilizing Transformer Training by Preventing Attention Entropy Collapse: https://github.com/apple/ml-sigma-reparam,https://paperswithcode.com/paper/stabilizing-transformer-training-by,https://da.gd/jMDEu
PwC:Trending,/haoshao-nku/ MCANet: Medical Image Segmentation with Multi-Scale Cross-Axis Attention: https://github.com/haoshao-nku/medical_seg,https://paperswithcode.com/paper/mcanet-medical-image-segmentation-with-multi,https://da.gd/wha8w
PwC:Trending,/zym-pku/ UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models: https://github.com/zym-pku/udifftext,https://paperswithcode.com/paper/udifftext-a-unified-framework-for-high,https://da.gd/JfteX
PwC:Trending,/microsoft/ PromptBench: A Unified Library for Evaluation of Large Language Models: https://github.com/microsoft/promptbench,https://paperswithcode.com/paper/promptbench-a-unified-library-for-evaluation,https://da.gd/V2M9
PwC:Trending,/opendilab/ LMDrive: Closed-Loop End-to-End Driving with Large Language Models: https://github.com/opendilab/lmdrive,https://paperswithcode.com/paper/lmdrive-closed-loop-end-to-end-driving-with,https://da.gd/5bQv
PwC:Trending,/martius-lab/ Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models: https://github.com/martius-lab/depRL,https://paperswithcode.com/paper/natural-and-robust-walking-using,https://da.gd/91P2NI
PwC:Trending,/microsoft/ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression: https://github.com/microsoft/LLMLingua,https://paperswithcode.com/paper/longllmlingua-accelerating-and-enhancing-llms,https://da.gd/4eiju
PwC:Trending,/myshell-ai/ OpenVoice: Versatile Instant Voice Cloning: https://github.com/myshell-ai/openvoice,https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning,https://da.gd/oOuXA
PwC:Trending,/leaplabthu/ Agent Attention: On the Integration of Softmax and Linear Attention: https://github.com/leaplabthu/agent-attention,https://paperswithcode.com/paper/agent-attention-on-the-integration-of-softmax,https://da.gd/yPMjy
