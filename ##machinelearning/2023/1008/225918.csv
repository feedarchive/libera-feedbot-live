feed,title,long_url,short_url
r/ML:50+,[R] Why is AdamW often superior to Adam with L2-Regularization in practice? The answer may lie in how weight decay balances updates across layers,https://redd.it/1731pcg,
