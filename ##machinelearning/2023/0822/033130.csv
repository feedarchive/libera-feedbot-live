feed,title,long_url,short_url
r/ML:50+,[D] Why fine tune a 65B LLM instead of using established task specific smaller models (~200 millions)?,https://redd.it/15xfesk,
