feed,title,long_url,short_url
PwC:Trending,/dao-ailab/ FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning: https://github.com/dao-ailab/flash-attention,https://paperswithcode.com/paper/flashattention-2-faster-attention-with-better,https://da.gd/w57GD2
PwC:Trending,/ailab-cvc/ Planting a SEED of Vision in Large Language Model: https://github.com/ailab-cvc/seed,https://paperswithcode.com/paper/planting-a-seed-of-vision-in-large-language,https://da.gd/Qv6Zn
PwC:Trending,/facebookresearch/ Llama 2: Open Foundation and Fine-Tuned Chat Models: https://github.com/facebookresearch/llama,https://paperswithcode.com/paper/llama-2-open-foundation-and-fine-tuned-chat,https://da.gd/Qn6ZG
PwC:Trending,/bigscience-workshop/ Petals: Collaborative Inference and Fine-tuning of Large Models: https://github.com/bigscience-workshop/petals,https://paperswithcode.com/paper/petals-collaborative-inference-and-fine,https://da.gd/FEg9M9
PwC:Trending,/IceClear/ Exploiting Diffusion Prior for Real-World Image Super-Resolution: https://github.com/IceClear/StableSR,https://paperswithcode.com/paper/exploiting-diffusion-prior-for-real-world,https://da.gd/bIXCSS
