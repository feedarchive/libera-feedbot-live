feed,title,long_url,short_url
r/ML:50+,[R] Instruct tuned Mixture of Experts Large Language Models significantly outperform dense counterparts. FLAN-MOE-32B surpasses FLAN-PALM-62B with a third of the compute,https://redd.it/14tch0b,
