feed,title,long_url,short_url
PwC:Latest,/plasmashen/ Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention: https://github.com/plasmashen/Dynamic-Attention,https://paperswithcode.com/paper/improving-the-robustness-of-transformer-based,https://da.gd/TWgox
PwC:Latest,/OPPO-Mente-Lab/ PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation: https://github.com/OPPO-Mente-Lab/PEA-Diffusion,https://paperswithcode.com/paper/pea-diffusion-parameter-efficient-adapter,https://da.gd/zLSLAo
