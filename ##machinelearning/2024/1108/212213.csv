feed,title,long_url,short_url
PwC:Trending,/tablegpt/ TableGPT2: A Large Multimodal Model with Tabular Data Integration: https://github.com/tablegpt/tablegpt-agent,https://paperswithcode.com/paper/tablegpt2-a-large-multimodal-model-with,https://da.gd/RRHdY
PwC:Trending,/ishohei220/ ADOPT: Modified Adam Can Converge with Any $Î²_2$ with the Optimal Rate: https://github.com/ishohei220/adopt,https://paperswithcode.com/paper/adopt-modified-adam-can-converge-with-any-b-2,https://da.gd/a0T19
PwC:Trending,/plageon/ HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems: https://github.com/plageon/HtmlRAG,https://paperswithcode.com/paper/htmlrag-html-is-better-than-plain-text-for,https://da.gd/QTfPZe
PwC:Trending,/donydchen/ MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views: https://github.com/donydchen/mvsplat360,https://paperswithcode.com/paper/mvsplat360-feed-forward-360-scene-synthesis,https://da.gd/86xdd
PwC:Trending,/THUDM/ WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning: https://github.com/THUDM/WebRL,https://paperswithcode.com/paper/webrl-training-llm-web-agents-via-self,https://da.gd/PeSOJ
PwC:Trending,/facebookresearch/ A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale: https://github.com/facebookresearch/optimizers/tree/main/distributed_shampoo,https://paperswithcode.com/paper/a-distributed-data-parallel-pytorch,https://da.gd/vD5ksy
PwC:Trending,/youngsheen/ Addressing Representation Collapse in Vector Quantized Models with One Linear Layer: https://github.com/youngsheen/SimVQ,https://paperswithcode.com/paper/addressing-representation-collapse-in-vector,https://da.gd/GFao
PwC:Trending,/THUDM/ AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents: https://github.com/THUDM/Android-Lab,https://paperswithcode.com/paper/androidlab-training-and-systematic,https://da.gd/45MF9L
