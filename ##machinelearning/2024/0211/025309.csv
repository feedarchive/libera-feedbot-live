feed,title,long_url,short_url
PwC:Trending,/time-series-foundation-models/ Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting: https://github.com/time-series-foundation-models/lag-llama,https://paperswithcode.com/paper/lag-llama-towards-foundation-models-for-time,https://da.gd/rnWD
PwC:Trending,/vahe1994/ Extreme Compression of Large Language Models via Additive Quantization: https://github.com/vahe1994/aqlm,https://paperswithcode.com/paper/extreme-compression-of-large-language-models,https://da.gd/RBWi
PwC:Trending,/stability-ai/ Fast Timing-Conditioned Latent Audio Diffusion: https://github.com/stability-ai/stable-audio-tools,https://paperswithcode.com/paper/fast-timing-conditioned-latent-audio,https://da.gd/jhX8
PwC:Trending,/aaronhuang-778/ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs: https://github.com/aaronhuang-778/billm,https://paperswithcode.com/paper/billm-pushing-the-limit-of-post-training,https://da.gd/frSXTs
PwC:Trending,/arplaboratory/ Learning to Fly in Seconds: https://github.com/arplaboratory/learning-to-fly,https://paperswithcode.com/paper/learning-to-fly-in-seconds,https://da.gd/vQ6Pj4
PwC:Trending,/kimmeen/ Position Paper: What Can Large Language Models Tell Us about Time Series Analysis: https://github.com/kimmeen/time-llm,https://paperswithcode.com/paper/position-paper-what-can-large-language-models,https://da.gd/nNDKXG
PwC:Trending,/mit-han-lab/ EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss: https://github.com/mit-han-lab/efficientvit,https://paperswithcode.com/paper/efficientvit-sam-accelerated-segment-anything,https://da.gd/ifdl9O
