feed,title,long_url,short_url
PwC:Latest,/whyNLP/ Layer-Condensed KV Cache for Efficient Inference of Large Language Models: https://github.com/whyNLP/LCKV,https://paperswithcode.com/paper/layer-condensed-kv-cache-for-efficient,https://da.gd/tCRGE
