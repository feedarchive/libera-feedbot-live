feed,title,long_url,short_url
PwC:Latest,/uic-indexlab/ Optimized Inference for 1.58-bit LLMs: A Time and Memory-Efficient Algorithm for Binary and Ternary Matrix Multiplication: https://github.com/uic-indexlab/rsr,https://paperswithcode.com/paper/optimized-inference-for-1-58-bit-llms-a-time,https://da.gd/lao4g
