feed,title,long_url,short_url
PwC:Trending,/foundationvision/ Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation: https://github.com/foundationvision/llamagen,https://paperswithcode.com/paper/autoregressive-model-beats-diffusion-llama,https://da.gd/BvhIJ
PwC:Trending,"/verazuo/ ""Do Anything Now"": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models: https://github.com/verazuo/jailbreak_llms",https://paperswithcode.com/paper/do-anything-now-characterizing-and-evaluating,https://da.gd/bIo3xD
PwC:Trending,/srameo/ Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis: https://github.com/srameo/le3d,https://paperswithcode.com/paper/lighting-every-darkness-with-3dgs-fast,https://da.gd/kjxjKL
PwC:Trending,/spcl/ Multi-Head RAG: Solving Multi-Aspect Problems with LLMs: https://github.com/spcl/mrag,https://paperswithcode.com/paper/multi-head-rag-solving-multi-aspect-problems,https://da.gd/SUuh6
PwC:Trending,/nerfstudio-project/ Mathematical Supplement for the $\texttt{gsplat}$ Library: https://github.com/nerfstudio-project/gsplat,https://paperswithcode.com/paper/mathematical-supplement-for-the-texttt-gsplat,https://da.gd/pxaZm
PwC:Trending,/apple/ Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training: https://github.com/apple/axlearn,https://paperswithcode.com/paper/revisiting-moe-and-dense-speed-accuracy,https://da.gd/UCbSh
PwC:Trending,/BytedanceSpeech/ Seed-TTS: A Family of High-Quality Versatile Speech Generation Models: https://github.com/BytedanceSpeech/seed-tts-eval,https://paperswithcode.com/paper/seed-tts-a-family-of-high-quality-versatile,https://da.gd/svwDI
PwC:Trending,"/agent-husky/ Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning: https://github.com/agent-husky/husky-v1",https://paperswithcode.com/paper/husky-a-unified-open-source-language-agent,https://da.gd/EN4AxW
PwC:Trending,/WUHU-G/ Recurrent Context Compression: Efficiently Expanding the Context Window of LLM: https://github.com/WUHU-G/RCC_Transformer,https://paperswithcode.com/paper/recurrent-context-compression-efficiently,https://da.gd/OjVcdg
