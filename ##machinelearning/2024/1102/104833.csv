feed,title,long_url,short_url
PwC:Trending,/hustvl/ Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving: https://github.com/hustvl/senna,https://paperswithcode.com/paper/senna-bridging-large-vision-language-models,https://da.gd/g6Honr
PwC:Trending,/haiyang-w/ TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters: https://github.com/haiyang-w/tokenformer,https://paperswithcode.com/paper/tokenformer-rethinking-transformer-scaling,https://da.gd/7yexi2
PwC:Trending,/apoorvkh/ $100K or 100 Days: Trade-offs when Pre-Training with Academic Resources: https://github.com/apoorvkh/academic-pretraining,https://paperswithcode.com/paper/100k-or-100-days-trade-offs-when-pre-training,https://da.gd/9sVp
PwC:Trending,/wjfu99/ MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector: https://github.com/wjfu99/mia-tuner,https://paperswithcode.com/paper/mia-tuner-adapting-large-language-models-as,https://da.gd/tdabo
PwC:Trending,/homebrewltd/ Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant: https://github.com/homebrewltd/ichigo,https://paperswithcode.com/paper/ichigo-mixed-modal-early-fusion-realtime,https://da.gd/sbSevw
PwC:Trending,/ali-vilab/ In-Context LoRA for Diffusion Transformers: https://github.com/ali-vilab/In-Context-LoRA,https://paperswithcode.com/paper/in-context-lora-for-diffusion-transformers,https://da.gd/sJJk
PwC:Trending,/bytedance/ ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference: https://github.com/bytedance/ShadowKV,https://paperswithcode.com/paper/shadowkv-kv-cache-in-shadows-for-high,https://da.gd/Mu4Hew
