feed,title,long_url,short_url
PwC:Trending,/shinechen1024/ Magic Clothing: Controllable Garment-Driven Image Synthesis: https://github.com/shinechen1024/magicclothing,https://paperswithcode.com/paper/magic-clothing-controllable-garment-driven,https://da.gd/BAOsx0
PwC:Trending,/event-ahu/ State Space Model for New-Generation Network Alternative to Transformers: A Survey: https://github.com/event-ahu/mamba_state_space_model_paper_list,https://paperswithcode.com/paper/state-space-model-for-new-generation-network,https://da.gd/mSErE
PwC:Trending,/zjukg/ MyGO: Discrete Modality Information as Fine-Grained Tokens for Multi-modal Knowledge Graph Completion: https://github.com/zjukg/mygo,https://paperswithcode.com/paper/mygo-discrete-modality-information-as-fine,https://da.gd/qDgF
PwC:Trending,/xuezhemax/ Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length: https://github.com/xuezhemax/megalodon,https://paperswithcode.com/paper/megalodon-efficient-llm-pretraining-and,https://da.gd/9VGn2
PwC:Trending,/siyan-zhao/ Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models: https://github.com/siyan-zhao/prepacking,https://paperswithcode.com/paper/prepacking-a-simple-method-for-fast,https://da.gd/IlKGz
PwC:Trending,/Recognito-Vision/ LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition: https://github.com/Recognito-Vision/Face-SDK-Linux-Demos,https://paperswithcode.com/paper/lafs-landmark-based-facial-self-supervised,https://da.gd/hb9gyM
PwC:Trending,/Beomi/ Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention: https://github.com/Beomi/InfiniTransformer,https://paperswithcode.com/paper/leave-no-context-behind-efficient-infinite,https://da.gd/ySgj
