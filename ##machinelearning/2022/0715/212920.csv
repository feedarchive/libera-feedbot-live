feed,title,long_url,short_url
r/ML:50+,[R] RWKV-3: Scaling RNN to 1.5B and Reach Transformer LM Performance (without using attention),https://redd.it/vzr6ie,
