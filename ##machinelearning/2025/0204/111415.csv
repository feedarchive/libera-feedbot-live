feed,title,long_url,short_url
PwC:Latest,/eleutherai/ Transcoders Beat Sparse Autoencoders for Interpretability: https://github.com/eleutherai/sae,https://paperswithcode.com/paper/transcoders-beat-sparse-autoencoders-for,https://da.gd/agQY9
PwC:Latest,/eleutherai/ Partially Rewriting a Transformer in Natural Language: https://github.com/eleutherai/sae-auto-interp,https://paperswithcode.com/paper/partially-rewriting-a-transformer-in-natural,https://da.gd/1ztOXJ
PwC:Latest,"/mustafakarabag/ Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game: https://github.com/mustafakarabag/llmchameleon",https://paperswithcode.com/paper/do-llms-strategically-reveal-conceal-and,https://da.gd/HHxD
PwC:Latest,/matchten/ Low-Rank Adapting Models for Sparse Autoencoders: https://github.com/matchten/lora-models-for-saes,https://paperswithcode.com/paper/low-rank-adapting-models-for-sparse,https://da.gd/XyxZh
PwC:Latest,/antoinedemathelin/ OneBatchPAM: A Fast and Frugal K-Medoids Algorithm: https://github.com/antoinedemathelin/obpam,https://paperswithcode.com/paper/onebatchpam-a-fast-and-frugal-k-medoids,https://da.gd/0NQZC
PwC:Latest,/orionw/ mFollowIR: a Multilingual Benchmark for Instruction Following in Retrieval: https://github.com/orionw/followir,https://paperswithcode.com/paper/mfollowir-a-multilingual-benchmark-for,https://da.gd/6mGcQ
