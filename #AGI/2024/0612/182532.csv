feed,title,long_url,short_url
r/AGI,Google study says fine-tuning an LLM linearly increases hallucinations? 😐,https://redd.it/1de5ty6,
r/AGI,"Human Centered Explainable AI (Mark Reidl, Georgia Tech)",https://redd.it/1de5y0b,
