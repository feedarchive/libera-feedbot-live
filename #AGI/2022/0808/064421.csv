feed,title,long_url,short_url
r/AGI,"The ""alignment problem"", in which intelligent machines have objectives aligned with those of humanity, weakens their intelligence",https://redd.it/wj0zne,
