feed,title,long_url,short_url
r/AGI,"BitNet: Scaling 1-bit Transformers for Large Language Models - Microsoft Research 2023 - Allows 1-Bit training from scratch while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods!",https://redd.it/17atkn1,
