feed,title,long_url,short_url
GN:S:RL,This Machine Learning Paper from Microsoft Proposes ChunkAttention: A Novel Self-Attention Module to Efficiently Manage KV Cache and Accelerate the Self-Attention Kernel for LLMs Inference - MarkTechPost,https://news.google.com/rss/articles/CBMi5AFodHRwczovL3d3dy5tYXJrdGVjaHBvc3QuY29tLzIwMjQvMDMvMDQvdGhpcy1tYWNoaW5lLWxlYXJuaW5nLXBhcGVyLWZyb20tbWljcm9zb2Z0LXByb3Bvc2VzLWNodW5rYXR0ZW50aW9uLWEtbm92ZWwtc2VsZi1hdHRlbnRpb24tbW9kdWxlLXRvLWVmZmljaWVudGx5LW1hbmFnZS1rdi1jYWNoZS1hbmQtYWNjZWxlcmF0ZS10aGUtc2VsZi1hdHRlbnRpb24ta2VybmVsLWZvci1sbG1zLWluZmVyZW5jZS_SAegBaHR0cHM6Ly93d3cubWFya3RlY2hwb3N0LmNvbS8yMDI0LzAzLzA0L3RoaXMtbWFjaGluZS1sZWFybmluZy1wYXBlci1mcm9tLW1pY3Jvc29mdC1wcm9wb3Nlcy1jaHVua2F0dGVudGlvbi1hLW5vdmVsLXNlbGYtYXR0ZW50aW9uLW1vZHVsZS10by1lZmZpY2llbnRseS1tYW5hZ2Uta3YtY2FjaGUtYW5kLWFjY2VsZXJhdGUtdGhlLXNlbGYtYXR0ZW50aW9uLWtlcm5lbC1mb3ItbGxtcy1pbmZlcmVuY2UvP2FtcA?oc=5,https://da.gd/Oc2zR8
