feed,title,long_url,short_url
ArXiv,Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions,https://arxiv.org/abs/2505.11614,
ArXiv,Learning from Less: Guiding Deep Reinforcement Learning with Differentiable Symbolic Planning,https://arxiv.org/abs/2505.11661,
ArXiv,Unveiling the Black Box: A Multi-Layer Framework for Explaining Reinforcement Learning-Based Cyber Agents,https://arxiv.org/abs/2505.11708,
ArXiv,Reinforcement Learning Finetunes Small Subnetworks in Large Language Models,https://arxiv.org/abs/2505.11711,
ArXiv,REMOR: Automated Peer Review Generation with LLM Reasoning and Multi-Objective Reinforcement Learning,https://arxiv.org/abs/2505.11718,
ArXiv,Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling,https://arxiv.org/abs/2505.11792,
ArXiv,Retrospex: Language Agent Meets Offline Reinforcement Learning Critic,https://arxiv.org/abs/2505.11807,
ArXiv,Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning,https://arxiv.org/abs/2505.11827,
ArXiv,VeriReason: Reinforcement Learning with Testbench Feedback for Reasoning-Enhanced Verilog Generation,https://arxiv.org/abs/2505.11849,
ArXiv,Integrating Model-based Control and RL for Sim2Real Transfer of Tight Insertion Policies,https://arxiv.org/abs/2505.11858,
ArXiv,Q-Policy: Quantum-Enhanced Policy Evaluation for Scalable Reinforcement Learning,https://arxiv.org/abs/2505.11862,
ArXiv,Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning,https://arxiv.org/abs/2505.11864,
ArXiv,RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving,https://arxiv.org/abs/2505.11893,
ArXiv,AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning,https://arxiv.org/abs/2505.11896,
ArXiv,LLM-guided DRL for Multi-tier LEO Satellite Networks with Hybrid FSO/RF Links,https://arxiv.org/abs/2505.11978,
ArXiv,Adaptive Resolving Methods for Reinforcement Learning with Function Approximations,https://arxiv.org/abs/2505.12037,
ArXiv,VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning,https://arxiv.org/abs/2505.12081,
ArXiv,Federated Deep Reinforcement Learning for Privacy-Preserving Robotic-Assisted Surgery,https://arxiv.org/abs/2505.12153,
ArXiv,Near-Optimal Sample Complexities of Divergence-based S-rectangular Distributionally Robust Reinforcement Learning,https://arxiv.org/abs/2505.12202,
ArXiv,Of Mice and Machines: A Comparison of Learning Between Real World Mice and RL Agents,https://arxiv.org/abs/2505.12204,
ArXiv,Imagination-Limited Q-Learning for Offline Reinforcement Learning,https://arxiv.org/abs/2505.12211,
ArXiv,Emergent Active Perception and Dexterity of Simulated Humanoids from Visual Reinforcement Learning,https://arxiv.org/abs/2505.12278,
ArXiv,Efficient RL Training for Reasoning Models via Length-Aware Optimization,https://arxiv.org/abs/2505.12284,
ArXiv,Enhancing Visual Grounding for GUI Agents via Self-Evolutionary Reinforcement Learning,https://arxiv.org/abs/2505.12370,
ArXiv,Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL via Graph Matching and Stepwise Reward,https://arxiv.org/abs/2505.12380,
ArXiv,Table-R1: Region-based Reinforcement Learning for Table Understanding,https://arxiv.org/abs/2505.12415,
ArXiv,Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning,https://arxiv.org/abs/2505.12432,
ArXiv,UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection,https://arxiv.org/abs/2505.12457,
ArXiv,A Finite-Sample Analysis of Distributionally Robust Average-Reward Reinforcement Learning,https://arxiv.org/abs/2505.12462,
ArXiv,Resolving Latency and Inventory Risk in Market Making with Reinforcement Learning,https://arxiv.org/abs/2505.12465,
ArXiv,UIShift: Enhancing VLM-based GUI Agents through Self-supervised Reinforcement Learning,https://arxiv.org/abs/2505.12493,
ArXiv,CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models,https://arxiv.org/abs/2505.12504,
ArXiv,Robust Reinforcement Learning-Based Locomotion for Resource-Constrained Quadrupeds with Exteroceptive Sensing,https://arxiv.org/abs/2505.12537,
ArXiv,Dual-Agent Reinforcement Learning for Automated Feature Generation,https://arxiv.org/abs/2505.12628,
ArXiv,SafeMove-RL: A Certifiable Reinforcement Learning Framework for Dynamic Motion Constraints in Trajectory Planning,https://arxiv.org/abs/2505.12648,
ArXiv,Counterfactual Explanations for Continuous Action Reinforcement Learning,https://arxiv.org/abs/2505.12701,
ArXiv,Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning,https://arxiv.org/abs/2505.12737,
ArXiv,Your Offline Policy is Not Trustworthy: Bilevel Reinforcement Learning for Sequential Portfolio Optimization,https://arxiv.org/abs/2505.12759,
ArXiv,ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL,https://arxiv.org/abs/2505.12768,
ArXiv,Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning,https://arxiv.org/abs/2505.12811,
ArXiv,Power Allocation for Delay Optimization in Device-to-Device Networks: A Graph Reinforcement Learning Approach,https://arxiv.org/abs/2505.12902,
ArXiv,Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs,https://arxiv.org/abs/2505.12929,
ArXiv,"Multi-parameter Control for the (1+($\lambda$,$\lambda$))-GA on OneMax via Deep Reinforcement Learning",https://arxiv.org/abs/2505.12982,
ArXiv,ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning,https://arxiv.org/abs/2505.12996,
ArXiv,Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs,https://arxiv.org/abs/2505.13026,
ArXiv,CAIM: Development and Evaluation of a Cognitive AI Memory Framework for Long-Term Interaction with Intelligent Agents,https://arxiv.org/abs/2505.13044,
ArXiv,Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning,https://arxiv.org/abs/2505.13144,
ArXiv,When a Reinforcement Learning Agent Encounters Unknown Unknowns,https://arxiv.org/abs/2505.13188,
ArXiv,Composing Dextrous Grasping and In-hand Manipulation via Scoring with a Reinforcement Learning Critic,https://arxiv.org/abs/2505.13253,
ArXiv,Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability,https://arxiv.org/abs/2505.13258,
ArXiv,Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning,https://arxiv.org/abs/2505.13261,
ArXiv,CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning,https://arxiv.org/abs/2505.13271,
ArXiv,Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning,https://arxiv.org/abs/2505.13372,
ArXiv,A Dataless Reinforcement Learning Approach to Rounding Hyperplane Optimization for Max-Cut,https://arxiv.org/abs/2505.13405,
ArXiv,G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning,https://arxiv.org/abs/2505.13426,
ArXiv,"Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards",https://arxiv.org/abs/2505.13445,
ArXiv,OmniDrones: An Efficient and Flexible Platform for Reinforcement Learning in Drone Control,https://arxiv.org/abs/2309.12825,
ArXiv,Tree-based Focused Web Crawling with Reinforcement Learning,https://arxiv.org/abs/2112.07620,
ArXiv,Counterfactual Q Learning via the Linear Buckley James Method for Longitudinal Survival Data,https://arxiv.org/abs/2505.12159,
