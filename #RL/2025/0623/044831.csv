feed,title,long_url,short_url
ArXiv,Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking,https://arxiv.org/abs/2506.15700,
ArXiv,Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning,https://arxiv.org/abs/2506.15701,
ArXiv,Steering Your Diffusion Policy with Latent Space Reinforcement Learning,https://arxiv.org/abs/2506.15799,
ArXiv,Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters,https://arxiv.org/abs/2506.15825,
ArXiv,Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search,https://arxiv.org/abs/2506.15880,
ArXiv,KARL: Kalman-Filter Assisted Reinforcement Learner for Dynamic Object Tracking and Grasping,https://arxiv.org/abs/2506.15945,
ArXiv,Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations,https://arxiv.org/abs/2506.16016,
ArXiv,A Lightweight RL-Driven Deep Unfolding Network for Robust WMMSE Precoding in Massive MU-MIMO-OFDM Systems,https://arxiv.org/abs/2506.16072,
ArXiv,GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning,https://arxiv.org/abs/2506.16141,
ArXiv,Multi-Task Lifelong Reinforcement Learning for Wireless Sensor Networks,https://arxiv.org/abs/2506.16254,
ArXiv,Goal-conditioned Hierarchical Reinforcement Learning for Sample-efficient and Safe Autonomous Driving at Intersections,https://arxiv.org/abs/2506.16336,
ArXiv,Data-Driven Policy Mapping for Safe RL-based Energy Management Systems,https://arxiv.org/abs/2506.16352,
ArXiv,Energy-Based Transfer for Reinforcement Learning,https://arxiv.org/abs/2506.16590,
ArXiv,Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces,https://arxiv.org/abs/2506.16608,
ArXiv,DRARL: Disengagement-Reason-Augmented Reinforcement Learning for Efficient Improvement of Autonomous Driving Policy,https://arxiv.org/abs/2506.16720,
ArXiv,Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation,https://arxiv.org/abs/2506.16753,
ArXiv,Reinforcement learning for hybrid charging stations planning and operation considering fixed and mobile chargers,https://arxiv.org/abs/2506.16764,
ArXiv,Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning,https://arxiv.org/abs/2506.16795,
ArXiv,RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought,https://arxiv.org/abs/2506.16796,
ArXiv,Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators,https://arxiv.org/abs/2506.17007,
ArXiv,Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment,https://arxiv.org/abs/2506.17029,
ArXiv,When Can Model-Free Reinforcement Learning be Enough for Thinking?,https://arxiv.org/abs/2506.17124,
ArXiv,Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity,https://arxiv.org/abs/2506.17155,
ArXiv,Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning,https://arxiv.org/abs/2506.17204,
ArXiv,BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning,https://arxiv.org/abs/2506.17211,
ArXiv,Quantum Fisher-Preconditioned Reinforcement Learning: From Single-Qubit Control to Rayleigh-Fading Link Adaptation,https://arxiv.org/abs/2506.15753,
