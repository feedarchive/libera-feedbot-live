feed,title,long_url,short_url
ArXiv,R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning,https://arxiv.org/abs/2509.20384,
ArXiv,Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations,https://arxiv.org/abs/2509.20478,
ArXiv,Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications,https://arxiv.org/abs/2509.20520,
ArXiv,Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning,https://arxiv.org/abs/2509.20541,
ArXiv,Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning,https://arxiv.org/abs/2509.20616,
ArXiv,CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning,https://arxiv.org/abs/2509.20712,
ArXiv,RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking,https://arxiv.org/abs/2509.20717,
ArXiv,SeamCrafte: Enhancing Mesh Seam Generation for Artist UV Unwrapping via Reinforcement Learning,https://arxiv.org/abs/2509.20725,
ArXiv,Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning,https://arxiv.org/abs/2509.20766,
ArXiv,Model-Based Reinforcement Learning under Random Observation Delays,https://arxiv.org/abs/2509.20869,
ArXiv,RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks,https://arxiv.org/abs/2509.20924,
ArXiv,"RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training",https://arxiv.org/abs/2509.21009,
ArXiv,ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning,https://arxiv.org/abs/2509.21010,
ArXiv,DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?,https://arxiv.org/abs/2509.21016,
ArXiv,Actor-Critic without Actor,https://arxiv.org/abs/2509.21022,
ArXiv,Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs,https://arxiv.org/abs/2509.21044,
ArXiv,MPC-based Deep Reinforcement Learning Method for Space Robotic Control with Fuel Sloshing Mitigation,https://arxiv.org/abs/2509.21045,
ArXiv,Task-Oriented Computation Offloading for Edge Inference: An Integrated Bayesian Optimization and Deep Reinforcement Learning Framework,https://arxiv.org/abs/2509.21090,
ArXiv,MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning,https://arxiv.org/abs/2509.21113,
ArXiv,Rich State Observations Empower Reinforcement Learning to Surpass PID: A Drone Ball Balancing Study,https://arxiv.org/abs/2509.21122,
ArXiv,Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning,https://arxiv.org/abs/2509.21126,
ArXiv,"RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",https://arxiv.org/abs/2509.21128,
ArXiv,Inverse Reinforcement Learning Using Just Classification and a Few Regressions,https://arxiv.org/abs/2509.21172,
ArXiv,AbideGym: Turning Static RL Worlds into Adaptive Challenges,https://arxiv.org/abs/2509.21234,
ArXiv,Tree Search for LLM Agent Reinforcement Learning,https://arxiv.org/abs/2509.21240,
ArXiv,"It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL",https://arxiv.org/abs/2509.21282,
