feed,title,long_url,short_url
ArXiv,Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning,https://arxiv.org/abs/2510.03259,
ArXiv,Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models,https://arxiv.org/abs/2510.03302,
ArXiv,On Architectures for Combining Reinforcement Learning and Model Predictive Control with Runtime Improvements,https://arxiv.org/abs/2510.03354,
ArXiv,Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^\pi$-Realizability and Concentrability,https://arxiv.org/abs/2510.03494,
ArXiv,D2 Actor Critic: Diffusion Actor Meets Distributional Critic,https://arxiv.org/abs/2510.03508,
ArXiv,RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models,https://arxiv.org/abs/2510.03515,
ArXiv,Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning,https://arxiv.org/abs/2510.03534,
ArXiv,Deep Reinforcement Learning for Multi-Agent Coordination,https://arxiv.org/abs/2510.03592,
ArXiv,Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning,https://arxiv.org/abs/2510.03669,
ArXiv,Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach,https://arxiv.org/abs/2510.03722,
ArXiv,TROLL: Trust Regions improve Reinforcement Learning for Large Language Models,https://arxiv.org/abs/2510.03817,
ArXiv,Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning,https://arxiv.org/abs/2510.03823,
ArXiv,Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration,https://arxiv.org/abs/2510.03865,
ArXiv,What Can You Do When You Have Zero Rewards During RL?,https://arxiv.org/abs/2510.03971,
ArXiv,Principled and Tractable RL for Reasoning with Diffusion Language Models,https://arxiv.org/abs/2510.04019,
ArXiv,Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models,https://arxiv.org/abs/2510.04020,
ArXiv,"From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents",https://arxiv.org/abs/2510.04076,
ArXiv,PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity,https://arxiv.org/abs/2510.04080,
ArXiv,Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees,https://arxiv.org/abs/2510.04088,
ArXiv,RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback,https://arxiv.org/abs/2510.04096,
ArXiv,WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning,https://arxiv.org/abs/2510.04097,
ArXiv,Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs,https://arxiv.org/abs/2510.04140,
ArXiv,Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation,https://arxiv.org/abs/2510.04168,
ArXiv,COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability,https://arxiv.org/abs/2510.04196,
ArXiv,"AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework",https://arxiv.org/abs/2510.04206,
ArXiv,Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales,https://arxiv.org/abs/2510.04272,
ArXiv,Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning,https://arxiv.org/abs/2510.04284,
ArXiv,Achieve Performatively Optimal Policy for Performative Reinforcement Learning,https://arxiv.org/abs/2510.04430,
ArXiv,Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners,https://arxiv.org/abs/2510.04454,
ArXiv,Wavelet Predictive Representations for Non-Stationary Reinforcement Learning,https://arxiv.org/abs/2510.04507,
ArXiv,Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets,https://arxiv.org/abs/2510.04555,
ArXiv,Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning,https://arxiv.org/abs/2510.04786,
ArXiv,Video Game Level Design as a Multi-Agent Reinforcement Learning Problem,https://arxiv.org/abs/2510.04862,
ArXiv,Model Predictive Control-Guided Reinforcement Learning for Implicit Balancing,https://arxiv.org/abs/2510.04868,
ArXiv,RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection,https://arxiv.org/abs/2510.04885,
ArXiv,MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning,https://arxiv.org/abs/2510.04935,
ArXiv,Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits,https://arxiv.org/abs/2510.04952,
ArXiv,"Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot",https://arxiv.org/abs/2510.05001,
ArXiv,Automaton Constrained Q-Learning,https://arxiv.org/abs/2510.05061,
ArXiv,Dissecting Larval Zebrafish Hunting using Deep Reinforcement Learning Trained RNN Agents,https://arxiv.org/abs/2510.03699,
ArXiv,"Quantizer Design for Finite Model Approximations, Model Learning, and Quantized Q-Learning for MDPs with Unbounded Spaces",https://arxiv.org/abs/2510.04355,
ArXiv,Algorithmic pricing with independent learners and relative experience replay,https://arxiv.org/abs/2102.09139,
ArXiv,Risk-Sensitive Option Market Making with Arbitrage-Free eSSVI Surfaces: A Constrained RL and Stochastic Control Bridge,https://arxiv.org/abs/2510.04569,
