feed,title,long_url,short_url
ArXiv,APRIL: API Synthesis with Automatic Prompt Optimization and Reinforcement Learning,https://arxiv.org/abs/2509.25196,
ArXiv,Reinforcement Learning-Guided Chain-of-Draft for Token-Efficient Code Generation,https://arxiv.org/abs/2509.25243,
ArXiv,Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning,https://arxiv.org/abs/2509.25267,
ArXiv,RL in the Wild: Characterizing RLVR Training in LLM Deployment,https://arxiv.org/abs/2509.25279,
ArXiv,Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using Deep Reinforcement Learning,https://arxiv.org/abs/2509.25284,
ArXiv,Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning,https://arxiv.org/abs/2509.25300,
ArXiv,Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning Models,https://arxiv.org/abs/2509.25402,
ArXiv,Polychromic Objectives for Reinforcement Learning,https://arxiv.org/abs/2509.25424,
ArXiv,CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation,https://arxiv.org/abs/2509.25443,
ArXiv,DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search,https://arxiv.org/abs/2509.25454,
ArXiv,PIPer: On-Device Environment Setup via Online Reinforcement Learning,https://arxiv.org/abs/2509.25455,
ArXiv,Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning,https://arxiv.org/abs/2509.25534,
ArXiv,Safe In-Context Reinforcement Learning,https://arxiv.org/abs/2509.25582,
ArXiv,Deep Reinforcement Learning-Based Precoding for Multi-RIS-Aided Multiuser Downlink Systems with Practical Phase Shift,https://arxiv.org/abs/2509.25661,
ArXiv,Boundary-to-Region Supervision for Offline Safe Reinforcement Learning,https://arxiv.org/abs/2509.25727,
ArXiv,Cooperative Autonomous Driving in Diverse Behavioral Traffic: A Heterogeneous Graph Reinforcement Learning Approach,https://arxiv.org/abs/2509.25751,
ArXiv,SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling,https://arxiv.org/abs/2509.25756,
ArXiv,TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning,https://arxiv.org/abs/2509.25760,
ArXiv,Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs,https://arxiv.org/abs/2509.25779,
ArXiv,Learning to Reason as Action Abstractions with Scalable Mid-Training RL,https://arxiv.org/abs/2509.25810,
ArXiv,Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation,https://arxiv.org/abs/2509.25849,
ArXiv,RL-Guided Data Selection for Language Model Finetuning,https://arxiv.org/abs/2509.25850,
ArXiv,Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space,https://arxiv.org/abs/2509.25876,
ArXiv,Mem-{\alpha}: Learning Memory Construction via Reinforcement Learning,https://arxiv.org/abs/2509.25911,
ArXiv,RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning,https://arxiv.org/abs/2509.25958,
ArXiv,R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning,https://arxiv.org/abs/2509.25987,
ArXiv,Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access,https://arxiv.org/abs/2509.26000,
ArXiv,Knowledge Defined Networking for 6G: A Reinforcement Learning Example for Resource Management,https://arxiv.org/abs/2509.26075,
ArXiv,Evolutionary Continuous Adaptive RL-Powered Co-Design for Humanoid Chin-Up Performance,https://arxiv.org/abs/2509.26082,
ArXiv,Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models,https://arxiv.org/abs/2509.26114,
ArXiv,Accelerating Transformers in Online RL,https://arxiv.org/abs/2509.26137,
ArXiv,Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning,https://arxiv.org/abs/2509.26383,
ArXiv,Extensions of Robbins-Siegmund Theorem with Applications in Reinforcement Learning,https://arxiv.org/abs/2509.26442,
ArXiv,Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces,https://arxiv.org/abs/2509.26594,
ArXiv,Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning,https://arxiv.org/abs/2509.26605,
ArXiv,Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models,https://arxiv.org/abs/2509.26628,
ArXiv,Apple: Toward General Active Perception via Reinforcement Learning,https://arxiv.org/abs/2505.06182,
