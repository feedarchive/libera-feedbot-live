feed,title,long_url,short_url
ArXiv,Synthetic Data RL: Task Definition Is All You Need,https://arxiv.org/abs/2505.17063,
ArXiv,Effective Reinforcement Learning for Reasoning in Language Models,https://arxiv.org/abs/2505.17218,
ArXiv,Backdoors in DRL: Four Environments Focusing on In-distribution Triggers,https://arxiv.org/abs/2505.17248,
ArXiv,Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning,https://arxiv.org/abs/2505.17249,
ArXiv,ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models,https://arxiv.org/abs/2505.17250,
ArXiv,A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety,https://arxiv.org/abs/2505.17342,
ArXiv,Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey,https://arxiv.org/abs/2505.17352,
ArXiv,Towards VM Rescheduling Optimization Through Deep Reinforcement Learning,https://arxiv.org/abs/2505.17359,
ArXiv,Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation,https://arxiv.org/abs/2505.17391,
ArXiv,Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning,https://arxiv.org/abs/2505.17439,
ArXiv,Co-Reinforcement Learning for Unified Multimodal Understanding and Generation,https://arxiv.org/abs/2505.17534,
ArXiv,RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning,https://arxiv.org/abs/2505.17540,
ArXiv,Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective,https://arxiv.org/abs/2505.17652,
ArXiv,QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning,https://arxiv.org/abs/2505.17667,
ArXiv,URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles,https://arxiv.org/abs/2505.17734,
ArXiv,Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning,https://arxiv.org/abs/2505.17749,
ArXiv,DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors,https://arxiv.org/abs/2505.17795,
ArXiv,Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning,https://arxiv.org/abs/2505.17830,
ArXiv,T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation,https://arxiv.org/abs/2505.17897,
ArXiv,Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL,https://arxiv.org/abs/2505.17952,
ArXiv,Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning,https://arxiv.org/abs/2505.17988,
ArXiv,Outcome-based Reinforcement Learning to Predict the Future,https://arxiv.org/abs/2505.17989,
ArXiv,Stable Reinforcement Learning for Efficient Reasoning,https://arxiv.org/abs/2505.18086,
ArXiv,Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL,https://arxiv.org/abs/2505.18098,
ArXiv,Bridging Supervised Learning and Reinforcement Learning in Math Reasoning,https://arxiv.org/abs/2505.18116,
ArXiv,One RL to See Them All: Visual Triple Unified Reinforcement Learning,https://arxiv.org/abs/2505.18129,
ArXiv,Offline Constrained Reinforcement Learning under Partial Data Coverage,https://arxiv.org/abs/2505.17506,
ArXiv,Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning,https://arxiv.org/abs/2410.15639,
ArXiv,Normalized Cut with Reinforcement Learning in Constrained Action Space,https://arxiv.org/abs/2505.13986,
