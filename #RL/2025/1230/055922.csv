feed,title,long_url,short_url
ArXiv,VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning,https://arxiv.org/abs/2512.22315,
ArXiv,Role-Based Fault Tolerance System for LLM RL Post-Training,https://arxiv.org/abs/2512.22492,
ArXiv,RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure,https://arxiv.org/abs/2512.22560,
ArXiv,FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution,https://arxiv.org/abs/2512.22647,
ArXiv,Optimal Regulation of Nonlinear Input-Affine Systems via an Integral Reinforcement Learning-Based State-Dependent Riccati Equation Approach,https://arxiv.org/abs/2512.22668,
ArXiv,TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning,https://arxiv.org/abs/2512.22824,
ArXiv,MARPO: A Reflective Policy Optimization for Multi Agent Reinforcement Learning,https://arxiv.org/abs/2512.22832,
ArXiv,AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning,https://arxiv.org/abs/2512.22857,
ArXiv,"Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks",https://arxiv.org/abs/2512.22860,
ArXiv,Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks,https://arxiv.org/abs/2512.22876,
ArXiv,SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning,https://arxiv.org/abs/2512.22895,
ArXiv,Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning,https://arxiv.org/abs/2512.22910,
ArXiv,Heterogeneity in Multi-Agent Reinforcement Learning,https://arxiv.org/abs/2512.22941,
ArXiv,Trust Region Masking for Long-Horizon LLM Reinforcement Learning,https://arxiv.org/abs/2512.23075,
ArXiv,Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning,https://arxiv.org/abs/2512.23087,
ArXiv,"Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients",https://arxiv.org/abs/2512.23090,
ArXiv,Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL,https://arxiv.org/abs/2512.23310,
ArXiv,CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation,https://arxiv.org/abs/2512.23333,
ArXiv,A Design Space for Intelligent Agents in Mixed-Initiative Visual Analytics,https://arxiv.org/abs/2512.23372,
ArXiv,Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following,https://arxiv.org/abs/2512.23457,
ArXiv,Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization,https://arxiv.org/abs/2512.23493,
ArXiv,Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning,https://arxiv.org/abs/2512.23515,
ArXiv,Bellman Calibration for V-Learning in Offline Reinforcement Learning,https://arxiv.org/abs/2512.23694,
ArXiv,Coarse Q-learning in Decision-Making: Indifference vs. Indeterminacy vs. Instability,https://arxiv.org/abs/2412.09321,
ArXiv,Reinforcement Learning for Optimal Stopping in POMDPs with Application to Quickest Change Detection,https://arxiv.org/abs/2512.22347,
