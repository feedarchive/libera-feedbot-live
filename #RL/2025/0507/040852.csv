feed,title,long_url,short_url
ArXiv,Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning,https://arxiv.org/abs/2505.03172,
ArXiv,VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making,https://arxiv.org/abs/2505.03181,
ArXiv,DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning,https://arxiv.org/abs/2505.03209,
ArXiv,Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach,https://arxiv.org/abs/2505.03230,
ArXiv,RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning,https://arxiv.org/abs/2505.03238,
ArXiv,Multi-Agent Deep Reinforcement Learning for Zonal Ancillary Market Coupling,https://arxiv.org/abs/2505.03288,
ArXiv,RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation,https://arxiv.org/abs/2505.03344,
ArXiv,Effective Reinforcement Learning Control using Conservative Soft Actor-Critic,https://arxiv.org/abs/2505.03356,
ArXiv,Multi-Agent Reinforcement Learning Scheduling to Support Low Latency in Teleoperated Driving,https://arxiv.org/abs/2505.03558,
ArXiv,Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation,https://arxiv.org/abs/2505.03586,
ArXiv,Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning,https://arxiv.org/abs/2505.03721,
ArXiv,Actor-Critics Can Achieve Optimal Sample Efficiency,https://arxiv.org/abs/2505.03710,
ArXiv,q-Learning in Continuous Time,https://arxiv.org/abs/2207.00713,
ArXiv,Aligning Data Selection with Performance: Performance-driven Reinforcement Learning for Active Learning in Object Detection,https://arxiv.org/abs/2310.08387,
ArXiv,HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment,https://arxiv.org/abs/2503.18991,
