feed,title,long_url,short_url
ArXiv,A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning,https://arxiv.org/abs/2507.08267,
ArXiv,Agent Safety Alignment via Reinforcement Learning,https://arxiv.org/abs/2507.08270,
ArXiv,Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training,https://arxiv.org/abs/2507.08284,
ArXiv,Intelligent Control of Spacecraft Reaction Wheel Attitude Using Deep Reinforcement Learning,https://arxiv.org/abs/2507.08366,
ArXiv,Online Pre-Training for Offline-to-Online Reinforcement Learning,https://arxiv.org/abs/2507.08387,
ArXiv,Age of Information Optimization in Laser-charged UAV-assisted IoT Networks: A Multi-agent Deep Reinforcement Learning Method,https://arxiv.org/abs/2507.08429,
ArXiv,SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2,https://arxiv.org/abs/2507.08548,
ArXiv,Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning,https://arxiv.org/abs/2507.08649,
ArXiv,elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings,https://arxiv.org/abs/2507.08705,
ArXiv,SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations,https://arxiv.org/abs/2507.08707,
ArXiv,Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data,https://arxiv.org/abs/2507.08761,
ArXiv,Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning,https://arxiv.org/abs/2507.08793,
