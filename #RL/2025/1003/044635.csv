feed,title,long_url,short_url
ArXiv,A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab,https://arxiv.org/abs/2510.01264,
ArXiv,Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance,https://arxiv.org/abs/2510.01269,
ArXiv,The Three Regimes of Offline-to-Online Reinforcement Learning,https://arxiv.org/abs/2510.01460,
ArXiv,Comparative Field Deployment of Reinforcement Learning and Model Predictive Control for Residential HVAC,https://arxiv.org/abs/2510.01475,
ArXiv,Off-Policy Reinforcement Learning with Anytime Safety Guarantees via Robust Safe Gradient Flow,https://arxiv.org/abs/2510.01492,
ArXiv,Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control,https://arxiv.org/abs/2510.01508,
ArXiv,Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs,https://arxiv.org/abs/2510.01527,
ArXiv,From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?,https://arxiv.org/abs/2510.01571,
ArXiv,AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning,https://arxiv.org/abs/2510.01586,
ArXiv,Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead,https://arxiv.org/abs/2510.01624,
ArXiv,PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning,https://arxiv.org/abs/2510.01715,
ArXiv,Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX,https://arxiv.org/abs/2510.01764,
ArXiv,Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning,https://arxiv.org/abs/2510.01824,
ArXiv,What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework,https://arxiv.org/abs/2510.01830,
ArXiv,SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with Reinforcement Learning,https://arxiv.org/abs/2510.01832,
ArXiv,Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning,https://arxiv.org/abs/2510.01833,
ArXiv,Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning,https://arxiv.org/abs/2510.01857,
ArXiv,Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning,https://arxiv.org/abs/2510.01932,
ArXiv,Reinforcement Learning with Action-Triggered Observations,https://arxiv.org/abs/2510.02149,
ArXiv,RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization,https://arxiv.org/abs/2510.02172,
ArXiv,GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning,https://arxiv.org/abs/2510.02180,
ArXiv,DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning,https://arxiv.org/abs/2510.02212,
ArXiv,The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models,https://arxiv.org/abs/2510.02230,
ArXiv,RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning,https://arxiv.org/abs/2510.02240,
ArXiv,How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning,https://arxiv.org/abs/2510.02265,
ArXiv,VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL,https://arxiv.org/abs/2510.02282,
ArXiv,Deep Hedging Under Non-Convexity: Limitations and a Case for AlphaZero,https://arxiv.org/abs/2510.01874,
