feed,title,long_url,short_url
ArXiv,Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning,https://arxiv.org/abs/2512.17912,
ArXiv,SuperFlow: Training Flow Matching Models with RL on the Fly,https://arxiv.org/abs/2512.17951,
ArXiv,"Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications",https://arxiv.org/abs/2512.18135,
ArXiv,Stable and Efficient Single-Rollout RL for Multimodal Reasoning,https://arxiv.org/abs/2512.18215,
ArXiv,Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems,https://arxiv.org/abs/2512.18317,
ArXiv,Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC),https://arxiv.org/abs/2512.18333,
ArXiv,Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism,https://arxiv.org/abs/2512.18336,
ArXiv,Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies,https://arxiv.org/abs/2512.18540,
ArXiv,Toward Training Superintelligent Software Agents through Self-Play SWE-RL,https://arxiv.org/abs/2512.18552,
ArXiv,Distributionally Robust Multi-Agent Reinforcement Learning for Intelligent Traffic Control,https://arxiv.org/abs/2512.18558,
ArXiv,ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning,https://arxiv.org/abs/2512.18571,
ArXiv,EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture,https://arxiv.org/abs/2512.18596,
ArXiv,Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning,https://arxiv.org/abs/2512.18604,
ArXiv,Offline Reinforcement Learning for End-to-End Autonomous Driving,https://arxiv.org/abs/2512.18662,
ArXiv,Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments,https://arxiv.org/abs/2512.18670,
ArXiv,A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models,https://arxiv.org/abs/2512.18730,
ArXiv,Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning,https://arxiv.org/abs/2512.18763,
ArXiv,Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation,https://arxiv.org/abs/2512.18957,
ArXiv,SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models,https://arxiv.org/abs/2512.19317,
ArXiv,First-Order Representation Languages for Goal-Conditioned RL,https://arxiv.org/abs/2512.19355,
ArXiv,Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation,https://arxiv.org/abs/2512.19361,
ArXiv,LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning,https://arxiv.org/abs/2512.19516,
ArXiv,CORE: Compensable Reward as a Catalyst for Improving Offline RL in Wireless Networks,https://arxiv.org/abs/2512.19671,
ArXiv,Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods,https://arxiv.org/abs/2512.17929,
ArXiv,Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics,https://arxiv.org/abs/2512.18892,
ArXiv,Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning,https://arxiv.org/abs/2301.11321,
