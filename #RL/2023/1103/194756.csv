feed,title,long_url,short_url
r/RL:10+,"""Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models"", Fu et al 2023 (self-attention learns higher-order gradient descent)",https://redd.it/17mjguf,
