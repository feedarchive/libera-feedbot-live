feed,title,long_url,short_url
ArXiv,Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning?,https://arxiv.org/abs/2312.00054v1,
ArXiv,Optimal Attack and Defense for Reinforcement Learning,https://arxiv.org/abs/2312.00198v1,
ArXiv,Sample Efficient Reinforcement Learning from Human Feedback via Active Exploration,https://arxiv.org/abs/2312.00267v1,
ArXiv,Age-Based Scheduling for Mobile Edge Computing: A Deep Reinforcement Learning Approach,https://arxiv.org/abs/2312.00279v1,
ArXiv,Efficient Off-Policy Safe Reinforcement Learning Using Trust Region Conditional Value at Risk,https://arxiv.org/abs/2312.00342v1,
ArXiv,TRC: Trust Region Conditional Value at Risk for Safe Reinforcement Learning,https://arxiv.org/abs/2312.00344v1,
ArXiv,Interior Point Constrained Reinforcement Learning with Global Convergence Guarantees,https://arxiv.org/abs/2312.00561v1,
ArXiv,Tracking Object Positions in Reinforcement Learning: A Metric for Keypoint Detection (extended version),https://arxiv.org/abs/2312.00592v1,
ArXiv,Safe Reinforcement Learning in Tensor Reproducing Kernel Hilbert Space,https://arxiv.org/abs/2312.00727v1,
ArXiv,Action valuation of on- and off-ball soccer players based on multi-agent deep reinforcement learning,https://arxiv.org/abs/2305.17886v2,
ArXiv,A Definition of Continual Reinforcement Learning,https://arxiv.org/abs/2307.11046v2,
ArXiv,RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback,https://arxiv.org/abs/2309.00267v2,
ArXiv,UAV-assisted Semantic Communication with Hybrid Action Reinforcement Learning,https://arxiv.org/abs/2309.16713v2,
