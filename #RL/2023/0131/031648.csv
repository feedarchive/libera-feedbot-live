feed,title,long_url,short_url
ArXiv,A Memory Efficient Deep Reinforcement Learning Approach For Snake Game Autonomous Agents,https://arxiv.org/abs/2301.11977v1,
ArXiv,Analyzing Robustness of the Deep Reinforcement Learning Algorithm in Ramp Metering Applications Considering False Data Injection Attack and Defense,https://arxiv.org/abs/2301.12036v1,
ArXiv,STEERING: Stein Information Directed Exploration for Model-Based Reinforcement Learning,https://arxiv.org/abs/2301.12038v1,
ArXiv,RCsearcher: Reaction Center Identification in Retrosynthesis via Deep Q-Learning,https://arxiv.org/abs/2301.12071v1,
ArXiv,Beyond Exponentially Fast Mixing in Average-Reward Reinforcement Learning via Multi-Level Monte Carlo Actor-Critic,https://arxiv.org/abs/2301.12083v1,
ArXiv,Turbulence control in plane Couette flow using low-dimensional neural ODE-based models and deep reinforcement learning,https://arxiv.org/abs/2301.12098v1,
ArXiv,APAC: Authorized Probability-controlled Actor-Critic For Offline Reinforcement Learning,https://arxiv.org/abs/2301.12130v1,
ArXiv,Towards Learning Rubik's Cube with N-tuple-based Reinforcement Learning,https://arxiv.org/abs/2301.12167v1,
ArXiv,SaFormer: A Conditional Sequence Modeling Approach to Offline Safe Reinforcement Learning,https://arxiv.org/abs/2301.12203v1,
ArXiv,On the Sample Complexity of Actor-Critic Method for Reinforcement Learning with Function Approximation,https://arxiv.org/abs/1910.08412v3,
ArXiv,Reinforcement Learning Based Temporal Logic Control with Soft Constraints Using Limit-deterministic Generalized Buchi Automata,https://arxiv.org/abs/2101.10284v5,
ArXiv,Dexterous Robotic Manipulation using Deep Reinforcement Learning and Knowledge Transfer for Complex Sparse Reward-based Tasks,https://arxiv.org/abs/2205.09683v2,
ArXiv,Dynamic Network Reconfiguration for Entropy Maximization using Deep Reinforcement Learning,https://arxiv.org/abs/2205.13578v2,
ArXiv,Finite-Time Analysis of Fully Decentralized Single-Timescale Actor-Critic,https://arxiv.org/abs/2206.05733v2,
ArXiv,PAC: Assisted Value Factorisation with Counterfactual Predictions in Multi-Agent Reinforcement Learning,https://arxiv.org/abs/2206.11420v3,
ArXiv,Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation,https://arxiv.org/abs/2206.11489v3,
ArXiv,"Q-Ensemble for Offline RL: Don't Scale the Ensemble, Scale the Batch Size",https://arxiv.org/abs/2211.11092v2,
ArXiv,Let Offline RL Flow: Training Conservative Agents in the Latent Space of Normalizing Flows,https://arxiv.org/abs/2211.11096v2,
ArXiv,Risk Sensitive Dead-end Identification in Safety-Critical Offline Reinforcement Learning,https://arxiv.org/abs/2301.05664v2,
ArXiv,Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons,https://arxiv.org/abs/2301.11270v2,
ArXiv,Reinforcement Learning from Diverse Human Preferences,https://arxiv.org/abs/2301.11774v2,
ArXiv,The impact of surplus sharing on the outcomes of specific investments under negotiated transfer pricing: An agent-based simulation with fuzzy Q-learning agents,https://arxiv.org/abs/2301.12255v1,
ArXiv,Singularity-aware Reinforcement Learning,https://arxiv.org/abs/2301.13152v1,
ArXiv,Autonomous Satellite Docking via Adaptive Optimal Output Rregulation: A Reinforcement Learning Approach,https://arxiv.org/abs/2301.12489v1,
ArXiv,Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning,https://arxiv.org/abs/2301.12593v1,
ArXiv,PAC-Bayesian Soft Actor-Critic Learning,https://arxiv.org/abs/2301.12776v1,
ArXiv,Improved Regret for Efficient Online Reinforcement Learning with Linear Function Approximation,https://arxiv.org/abs/2301.13087v1,
