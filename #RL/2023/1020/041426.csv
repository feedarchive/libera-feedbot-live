feed,title,long_url,short_url
PwC:Latest,/pku-alignment/ Safe RLHF: Safe Reinforcement Learning from Human Feedback: https://github.com/pku-alignment/safe-rlhf,https://paperswithcode.com/paper/safe-rlhf-safe-reinforcement-learning-from,https://da.gd/NRYJ
