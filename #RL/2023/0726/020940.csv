feed,title,long_url,short_url
ArXiv,Pilot Performance modeling via observer-based inverse reinforcement learning,https://arxiv.org/abs/2307.13150v1,
ArXiv,Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching,https://arxiv.org/abs/2307.13158v1,
ArXiv,Counterfactual Explanation Policies in RL,https://arxiv.org/abs/2307.13192v1,
ArXiv,Submodular Reinforcement Learning,https://arxiv.org/abs/2307.13372v1,
ArXiv,Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning,https://arxiv.org/abs/2307.13415v1,
ArXiv,Deep Reinforcement Learning for Robust Goal-Based Wealth Management,https://arxiv.org/abs/2307.13501v1,
ArXiv,Settling the Sample Complexity of Online Reinforcement Learning,https://arxiv.org/abs/2307.13586v1,
ArXiv,Safety Margins for Reinforcement Learning,https://arxiv.org/abs/2307.13642v1,
ArXiv,Tensor and Matrix Low-Rank Value-Function Approximation in Reinforcement Learning,https://arxiv.org/abs/2201.09736v2,
ArXiv,Deep Reinforcement Learning-Assisted Federated Learning for Robust Short-term Utility Demand Forecasting in Electricity Wholesale Markets,https://arxiv.org/abs/2206.11715v2,
