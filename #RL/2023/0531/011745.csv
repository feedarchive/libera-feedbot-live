feed,title,long_url,short_url
ArXiv,Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback,https://arxiv.org/abs/2305.18341v1,
ArXiv,Potential-based Credit Assignment for Cooperative RL-based Testing of Autonomous Vehicles,https://arxiv.org/abs/2305.18380v1,
ArXiv,Sample Complexity of Variance-reduced Distributionally Robust Q-learning,https://arxiv.org/abs/2305.18420v1,
ArXiv,GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning,https://arxiv.org/abs/2305.18427v1,
ArXiv,Cross-Entropy Estimators for Sequential Experiment Design with Reinforcement Learning,https://arxiv.org/abs/2305.18435v1,
ArXiv,Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism,https://arxiv.org/abs/2305.18438v1,
ArXiv,Off-Policy RL Algorithms Can be Sample-Efficient for Continuous Control via Sample Multiple Reuse,https://arxiv.org/abs/2305.18443v1,
ArXiv,Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning,https://arxiv.org/abs/2305.18459v1,
ArXiv,A Hybrid Framework of Reinforcement Learning and Convex Optimization for UAV-Based Autonomous Metaverse Data Collection,https://arxiv.org/abs/2305.18481v1,
ArXiv,Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning,https://arxiv.org/abs/2305.18499v1,
ArXiv,DoMo-AC: Doubly Multi-step Off-policy Actor-Critic Algorithm,https://arxiv.org/abs/2305.18501v1,
ArXiv,How to Query Human Feedback Efficiently in RL?,https://arxiv.org/abs/2305.18505v1,
ArXiv,RLAD: Reinforcement Learning from Pixels for Autonomous Driving in Urban Environments,https://arxiv.org/abs/2305.18510v1,
ArXiv,RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents,https://arxiv.org/abs/2208.06448v3,
ArXiv,PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm,https://arxiv.org/abs/2208.07914v3,
ArXiv,PAC-Bayesian Soft Actor-Critic Learning,https://arxiv.org/abs/2301.12776v2,
ArXiv,Why Target Networks Stabilise Temporal Difference Methods,https://arxiv.org/abs/2302.12537v2,
ArXiv,Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling,https://arxiv.org/abs/2304.05365v5,
ArXiv,Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning,https://arxiv.org/abs/2304.12824v2,
ArXiv,Video Prediction Models as Rewards for Reinforcement Learning,https://arxiv.org/abs/2305.14343v2,
ArXiv,SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning,https://arxiv.org/abs/2305.15486v2,
ArXiv,The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning,https://arxiv.org/abs/2305.15703v2,
ArXiv,Policy Synthesis and Reinforcement Learning for Discounted LTL,https://arxiv.org/abs/2305.17115v2,
ArXiv,No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions,https://arxiv.org/abs/2305.17380v2,
ArXiv,Centralised rehearsal of decentralised cooperation: Multi-agent reinforcement learning for the scalable coordination of residential energy flexibility,https://arxiv.org/abs/2305.18875v1,
ArXiv,Policy Optimization for Continuous Reinforcement Learning,https://arxiv.org/abs/2305.18901v1,
