feed,title,long_url,short_url
ArXiv,Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning,https://arxiv.org/abs/2303.10180v1,
ArXiv,Prevalence of Code Smells in Reinforcement Learning Projects,https://arxiv.org/abs/2303.10236v1,
ArXiv,Mobile Edge Adversarial Detection for Digital Twinning to the Metaverse with Deep Reinforcement Learning,https://arxiv.org/abs/2303.10288v1,
ArXiv,Play to Earn in the Metaverse with Mobile Edge Computing over Wireless Networks: A Deep Reinforcement Learning Approach,https://arxiv.org/abs/2303.10289v1,
ArXiv,Interpretable Reinforcement Learning via Neural Additive Models for Inventory Management,https://arxiv.org/abs/2303.10382v1,
ArXiv,CLIP4MC: An RL-Friendly Vision-Language Model for Minecraft,https://arxiv.org/abs/2303.10571v1,
ArXiv,Falsification-Based Robust Adversarial Reinforcement Learning,https://arxiv.org/abs/2007.00691v3,
ArXiv,Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis,https://arxiv.org/abs/2102.06548v4,
ArXiv,Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL,https://arxiv.org/abs/2106.05087v5,
ArXiv,Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective,https://arxiv.org/abs/2203.06865v3,
ArXiv,Reward Reports for Reinforcement Learning,https://arxiv.org/abs/2204.10817v3,
ArXiv,Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning,https://arxiv.org/abs/2206.02670v3,
ArXiv,Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework,https://arxiv.org/abs/2207.01955v2,
ArXiv,Active Exploration for Inverse Reinforcement Learning,https://arxiv.org/abs/2207.08645v3,
ArXiv,Minimizing Human Assistance: Augmenting a Single Demonstration for Deep Reinforcement Learning,https://arxiv.org/abs/2209.11275v2,
ArXiv,Safe Exploration Method for Reinforcement Learning under Existence of Disturbance,https://arxiv.org/abs/2209.15452v2,
ArXiv,ADLight: A Universal Approach of Traffic Signal Control with Augmented Data Using Reinforcement Learning,https://arxiv.org/abs/2210.13378v2,
ArXiv,Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons,https://arxiv.org/abs/2301.11270v3,
ArXiv,Deep reinforcement learning for the olfactory search POMDP: a quantitative benchmark,https://arxiv.org/abs/2302.00706v2,
ArXiv,Local Environment Poisoning Attacks on Federated Reinforcement Learning,https://arxiv.org/abs/2303.02725v2,
ArXiv,Computably Continuous Reinforcement-Learning Objectives are PAC-learnable,https://arxiv.org/abs/2303.05518v2,
ArXiv,Active hypothesis testing in unknown environments using recurrent neural networks and model free reinforcement learning,https://arxiv.org/abs/2303.10623v1,
ArXiv,"Multi-Agent Reinforcement Learning via Mean Field Control: Common Noise, Major Agents and Approximation Properties",https://arxiv.org/abs/2303.10665v1,
ArXiv,Improved Sample Complexity for Reward-free Reinforcement Learning under Low-rank MDPs,https://arxiv.org/abs/2303.10859v1,
