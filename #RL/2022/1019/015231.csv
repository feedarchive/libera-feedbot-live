feed,title,long_url,short_url
ArXiv,Sufficient Exploration for Convex Q-learning,https://arxiv.org/abs/2210.09409v1,
ArXiv,CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations,https://arxiv.org/abs/2210.09496v1,
ArXiv,Deep Black-Box Reinforcement Learning with Movement Primitives,https://arxiv.org/abs/2210.09622v1,
ArXiv,RPM: Generalizable Behaviors for Multi-Agent Reinforcement Learning,https://arxiv.org/abs/2210.09646v1,
ArXiv,Finite-time analysis of single-timescale actor-critic,https://arxiv.org/abs/2210.09921v1,
ArXiv,Rethinking Value Function Learning for Generalization in Reinforcement Learning,https://arxiv.org/abs/2210.09960v1,
ArXiv,Global Optimality and Finite Sample Analysis of Softmax Off-Policy Actor Critic under State Distribution Mismatch,https://arxiv.org/abs/2111.02997v2,
ArXiv,DOPE: Doubly Optimistic and Pessimistic Exploration for Safe Reinforcement Learning,https://arxiv.org/abs/2112.00885v3,
ArXiv,Factored Adaptation for Non-Stationary Reinforcement Learning,https://arxiv.org/abs/2203.16582v2,
ArXiv,Anchor-Changing Regularized Natural Policy Gradient for Multi-Objective Reinforcement Learning,https://arxiv.org/abs/2206.05357v2,
ArXiv,Bootstrapped Transformer for Offline Reinforcement Learning,https://arxiv.org/abs/2206.08569v2,
ArXiv,Robust Reinforcement Learning using Offline Data,https://arxiv.org/abs/2208.05129v2,
ArXiv,Honor of Kings Arena: an Environment for Generalization in Competitive Reinforcement Learning,https://arxiv.org/abs/2209.08483v3,
ArXiv,ToupleGDD: A Fine-Designed Solution of Influence Maximization by Deep Reinforcement Learning,https://arxiv.org/abs/2210.07500v2,
