feed,title,long_url,short_url
ArXiv,DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning,https://arxiv.org/abs/2602.00983,
ArXiv,Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning,https://arxiv.org/abs/2602.00994,
ArXiv,Reliable Use of Lemmas via Eligibility Reasoning and Section$-$Aware Reinforcement Learning,https://arxiv.org/abs/2602.00998,
ArXiv,ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning,https://arxiv.org/abs/2602.01003,
ArXiv,"Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning",https://arxiv.org/abs/2602.01058,
ArXiv,Resilience Optimization in 6G and Beyond Integrated Satellite-Terrestrial Networks: A Deep Reinforcement Learning Approach,https://arxiv.org/abs/2602.01102,
ArXiv,Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach,https://arxiv.org/abs/2602.01131,
ArXiv,PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning,https://arxiv.org/abs/2602.01156,
ArXiv,Sample Efficient Active Algorithms for Offline Reinforcement Learning,https://arxiv.org/abs/2602.01260,
ArXiv,Reinforcement Learning for Active Perception in Autonomous Navigation,https://arxiv.org/abs/2602.01266,
ArXiv,Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics,https://arxiv.org/abs/2602.01270,
ArXiv,What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom,https://arxiv.org/abs/2602.01334,
ArXiv,CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering,https://arxiv.org/abs/2602.01348,
ArXiv,When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning,https://arxiv.org/abs/2602.01365,
ArXiv,PromptRL: Prompt Matters in RL for Flow-Based Image Generation,https://arxiv.org/abs/2602.01382,
ArXiv,The Enhanced Physics-Informed Kolmogorov-Arnold Networks: Applications of Newton's Laws in Financial Deep Reinforcement Learning (RL) Algorithms,https://arxiv.org/abs/2602.01388,
ArXiv,Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum,https://arxiv.org/abs/2602.01505,
ArXiv,Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training,https://arxiv.org/abs/2602.01511,
ArXiv,A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning,https://arxiv.org/abs/2602.01523,
ArXiv,Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning,https://arxiv.org/abs/2602.01528,
ArXiv,Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards,https://arxiv.org/abs/2602.01601,
ArXiv,Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching,https://arxiv.org/abs/2602.01606,
ArXiv,Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning,https://arxiv.org/abs/2602.01649,
ArXiv,FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning,https://arxiv.org/abs/2602.01664,
ArXiv,TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning,https://arxiv.org/abs/2602.01665,
ArXiv,Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner,https://arxiv.org/abs/2602.01705,
ArXiv,RFS: Reinforcement learning with Residual flow steering for dexterous manipulation,https://arxiv.org/abs/2602.01789,
ArXiv,Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning,https://arxiv.org/abs/2602.01853,
ArXiv,VLM-Guided Experience Replay,https://arxiv.org/abs/2602.01915,
ArXiv,Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models,https://arxiv.org/abs/2602.01970,
ArXiv,FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification,https://arxiv.org/abs/2602.02055,
ArXiv,Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning,https://arxiv.org/abs/2602.02098,
ArXiv,ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning,https://arxiv.org/abs/2602.02150,
ArXiv,ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning,https://arxiv.org/abs/2602.02192,
ArXiv,Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL,https://arxiv.org/abs/2602.02236,
ArXiv,Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management,https://arxiv.org/abs/2602.02283,
ArXiv,David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning,https://arxiv.org/abs/2602.02395,
ArXiv,World-Gymnast: Training Robots with Reinforcement Learning in a World Model,https://arxiv.org/abs/2602.02454,
ArXiv,Expanding the Capabilities of Reinforcement Learning via Text Feedback,https://arxiv.org/abs/2602.02482,
ArXiv,"RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",https://arxiv.org/abs/2602.02488,
ArXiv,ProDCARL: Reinforcement Learning-Aligned Diffusion Models for De Novo Antimicrobial Peptide Design,https://arxiv.org/abs/2602.00157,
ArXiv,Reinforcement Learning for Control Systems with Time Delays: A Comprehensive Survey,https://arxiv.org/abs/2602.00399,
ArXiv,Action-Free Offline-to-Online RL via Discretised State Policies,https://arxiv.org/abs/2602.00629,
ArXiv,A Gait Driven Reinforcement Learning Framework for Humanoid Robots,https://arxiv.org/abs/2506.08416,
ArXiv,Adaptive Shielding for Safe Reinforcement Learning under Hidden-Parameter Dynamics Shifts,https://arxiv.org/abs/2506.11033,
ArXiv,A Forensic Analysis of Synthetic Data in RL: Diagnosing and Solving Algorithmic Failures in Model-Based Policy Optimization,https://arxiv.org/abs/2510.01457,
ArXiv,Model-Based Data-Efficient and Robust Reinforcement Learning,https://arxiv.org/abs/2602.00630,
ArXiv,Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems,https://arxiv.org/abs/2602.00027,
ArXiv,"Learning Safety-Guaranteed, Non-Greedy Control Barrier Functions Using Reinforcement Learning",https://arxiv.org/abs/2602.00366,
ArXiv,"Evaluation of Electricity Market Clearing Mechanisms via Reinforcement Learning: Prices, Remuneration and Competitive Dynamics",https://arxiv.org/abs/2602.01392,
ArXiv,Cumulative Treatment Effect Testing under Continuous Time Reinforcement Learning,https://arxiv.org/abs/2602.02246,
ArXiv,Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding,https://arxiv.org/abs/2602.00781,
