feed,title,long_url,short_url
ArXiv,LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach,https://arxiv.org/abs/2601.04208,
ArXiv,Cross-Language Speaker Attribute Prediction Using MIL and RL,https://arxiv.org/abs/2601.04257,
ArXiv,Making Tunable Parameters State-Dependent in Weather and Climate Models with Reinforcement Learning,https://arxiv.org/abs/2601.04268,
ArXiv,Online Action-Stacking Improves Reinforcement Learning Performance for Air Traffic Control,https://arxiv.org/abs/2601.04287,
ArXiv,Survival Dynamics of Neural and Programmatic Policies in Evolutionary Reinforcement Learning,https://arxiv.org/abs/2601.04365,
ArXiv,Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces,https://arxiv.org/abs/2601.04401,
ArXiv,Rate or Fate? RLV$^\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards,https://arxiv.org/abs/2601.04411,
ArXiv,Improving and Accelerating Offline RL in Large Discrete Action Spaces with Structured Policy Initialization,https://arxiv.org/abs/2601.04441,
ArXiv,Multiagent Reinforcement Learning with Neighbor Action Estimation,https://arxiv.org/abs/2601.04511,
ArXiv,TSSR: Two-Stage Swap-Reward-Driven Reinforcement Learning for Character-Level SMILES Generation,https://arxiv.org/abs/2601.04521,
ArXiv,GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence,https://arxiv.org/abs/2601.04525,
ArXiv,"Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",https://arxiv.org/abs/2601.04582,
ArXiv,Optimizing Path Planning using Deep Reinforcement Learning for UGVs in Precision Agriculture,https://arxiv.org/abs/2601.04668,
ArXiv,Learning Dynamics in RL Post-Training for Language Models,https://arxiv.org/abs/2601.04670,
ArXiv,Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning,https://arxiv.org/abs/2601.04672,
ArXiv,Tape: A Cellular Automata Benchmark for Evaluating Rule-Shift Generalization in Reinforcement Learning,https://arxiv.org/abs/2601.04695,
ArXiv,TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning,https://arxiv.org/abs/2601.04698,
ArXiv,ThinkDrive: Chain-of-Thought Guided Progressive Reinforcement Learning Fine-Tuning for Autonomous Driving,https://arxiv.org/abs/2601.04714,
ArXiv,Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models,https://arxiv.org/abs/2601.04731,
ArXiv,Thinking-Based Non-Thinking: Solving the Reward Hacking Problem in Training Hybrid Reasoning Models via Reinforcement Learning,https://arxiv.org/abs/2601.04805,
ArXiv,Intelligent resource allocation in wireless networks via deep reinforcement learning,https://arxiv.org/abs/2601.04842,
ArXiv,Flexible Manufacturing Systems Intralogistics: Dynamic Optimization of AGVs and Tool Sharing Using Coloured-Timed Petri Nets and Actor-Critic RL with Actions Masking,https://arxiv.org/abs/2601.04887,
ArXiv,On the Hidden Objective Biases of Group-based Reinforcement Learning,https://arxiv.org/abs/2601.05002,
ArXiv,From Idea to Co-Creation: A Planner-Actor-Critic Framework for Agent Augmented 3D Modeling,https://arxiv.org/abs/2601.05016,
ArXiv,Safe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art,https://arxiv.org/abs/2601.05152,
ArXiv,SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning,https://arxiv.org/abs/2601.05187,
ArXiv,GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization,https://arxiv.org/abs/2601.05242,
ArXiv,RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes,https://arxiv.org/abs/2601.05249,
ArXiv,Deep Reinforcement Learning for Optimum Order Execution: Mitigating Risk and Maximizing Returns,https://arxiv.org/abs/2601.04896,
