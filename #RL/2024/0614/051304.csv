feed,title,long_url,short_url
ArXiv,AlphaZeroES: Direct score maximization outperforms planning loss minimization,https://arxiv.org/abs/2406.08687,
ArXiv,When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search,https://arxiv.org/abs/2406.08705,
ArXiv,RL-JACK: Reinforcement Learning-powered Black-box Jailbreaking Attack against LLMs,https://arxiv.org/abs/2406.08725,
ArXiv,Current applications and potential future directions of reinforcement learning-based Digital Twins in agriculture,https://arxiv.org/abs/2406.08854,
ArXiv,CIMRL: Combining IMitiation and Reinforcement Learning for Safe Autonomous Driving,https://arxiv.org/abs/2406.08878,
ArXiv,XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning,https://arxiv.org/abs/2406.08973,
ArXiv,CUER: Corrected Uniform Experience Replay for Off-Policy Continuous Deep Reinforcement Learning Algorithms,https://arxiv.org/abs/2406.09030,
ArXiv,Latent Assistance Networks: Rediscovering Hyperbolic Tangents in RL,https://arxiv.org/abs/2406.09079,
ArXiv,DiffPoGAN: Diffusion Policies with Generative Adversarial Networks for Offline Reinforcement Learning,https://arxiv.org/abs/2406.09089,
ArXiv,Is Value Learning Really the Main Bottleneck in Offline RL?,https://arxiv.org/abs/2406.09329,
ArXiv,Robust Knowledge Transfer in Tiered Reinforcement Learning,https://arxiv.org/abs/2302.05534,
ArXiv,Anytime-Constrained Reinforcement Learning,https://arxiv.org/abs/2311.05511,
ArXiv,A Minimaximalist Approach to Reinforcement Learning from Human Feedback,https://arxiv.org/abs/2401.04056,
ArXiv,Provably Efficient Exploration in Quantum Reinforcement Learning with Logarithmic Worst-Case Regret,https://arxiv.org/abs/2302.10796,
