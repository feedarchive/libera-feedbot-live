feed,title,long_url,short_url
ArXiv,Meta Reinforcement Learning for Strategic IoT Deployments Coverage in Disaster-Response UAV Swarms,https://arxiv.org/abs/2401.11118v1,
ArXiv,Obstacle-Aware Navigation of Soft Growing Robots via Deep Reinforcement Learning,https://arxiv.org/abs/2401.11203v1,
ArXiv,Measuring Policy Distance for Multi-Agent Reinforcement Learning,https://arxiv.org/abs/2401.11257v1,
ArXiv,New Versions of Gradient Temporal Difference Learning,https://arxiv.org/abs/2109.04033v4,
ArXiv,Deep Reinforcement Learning with Swin Transformers,https://arxiv.org/abs/2206.15269v3,
ArXiv,Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning,https://arxiv.org/abs/2303.05479v4,
ArXiv,Explaining RL Decisions with Trajectories,https://arxiv.org/abs/2305.04073v2,
ArXiv,Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach,https://arxiv.org/abs/2307.06564v2,
ArXiv,FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning,https://arxiv.org/abs/2307.13716v3,
ArXiv,Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs,https://arxiv.org/abs/2309.13365v3,
ArXiv,Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents,https://arxiv.org/abs/2311.13373v4,
ArXiv,Beyond Expected Return: Accounting for Policy Reproducibility when Evaluating Reinforcement Learning Algorithms,https://arxiv.org/abs/2312.07178v2,
ArXiv,MoMA: Model-based Mirror Ascent for Offline Reinforcement Learning,https://arxiv.org/abs/2401.11380v1,
ArXiv,Information-Theoretic State Variable Selection for Reinforcement Learning,https://arxiv.org/abs/2401.11512v1,
ArXiv,Mitigating Covariate Shift in Misspecified Regression with Applications to Reinforcement Learning,https://arxiv.org/abs/2401.12216v1,
