feed,title,long_url,short_url
ArXiv,Learning to Watermark LLM-generated Text via Reinforcement Learning,https://arxiv.org/abs/2403.10553,
ArXiv,EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning,https://arxiv.org/abs/2403.10692,
ArXiv,PERL: Parameter Efficient Reinforcement Learning from Human Feedback,https://arxiv.org/abs/2403.10704,
ArXiv,Leveraging Symmetries in Gaits for Reinforcement Learning: A Case Study on Quadrupedal Gaits,https://arxiv.org/abs/2403.10723,
ArXiv,Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning,https://arxiv.org/abs/2403.10761,
ArXiv,Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games,https://arxiv.org/abs/2403.10794,
ArXiv,Deep Reinforcement Learning-based Large-scale Robot Exploration,https://arxiv.org/abs/2403.10833,
ArXiv,Reinforcement Learning with Options,https://arxiv.org/abs/2403.10855,
ArXiv,ViSaRL: Visual Reinforcement Learning Guided by Human Saliency,https://arxiv.org/abs/2403.10940,
ArXiv,A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems,https://arxiv.org/abs/2403.10996,
ArXiv,PyroTrack: Belief-Based Deep Reinforcement Learning Path Planning for Aerial Wildfire Monitoring in Partially Observable Environments,https://arxiv.org/abs/2403.11095,
ArXiv,Phasic Diversity Optimization for Population-Based Reinforcement Learning,https://arxiv.org/abs/2403.11114,
ArXiv,Continuous Jumping of a Parallel Wire-Driven Monopedal Robot RAMIEL Using Reinforcement Learning,https://arxiv.org/abs/2403.11205,
ArXiv,Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective,https://arxiv.org/abs/2403.11345,
ArXiv,Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making,https://arxiv.org/abs/2403.11432,
ArXiv,RL en Markov Games with Independent Function Approximation: Improved Sample Complexity Bound under the Local Access Model,https://arxiv.org/abs/2403.11544,
ArXiv,Reinforcement Learning with Token-level Feedback for Controllable Text Generation,https://arxiv.org/abs/2403.11558,
ArXiv,Offline Multitask Representation Learning for Reinforcement Learning,https://arxiv.org/abs/2403.11574,
ArXiv,The Value of Reward Lookahead in Reinforcement Learning,https://arxiv.org/abs/2403.11637,
ArXiv,Locomotion Generation for a Rat Robot based on Environmental Changes via Reinforcement Learning,https://arxiv.org/abs/2403.11788,
ArXiv,Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay,https://arxiv.org/abs/2403.11852,
ArXiv,Distill2Explain: Differentiable decision trees for explainable reinforcement learning in energy application controllers,https://arxiv.org/abs/2403.11907,
ArXiv,Single-Agent Actor Critic for Decentralized Cooperative Driving,https://arxiv.org/abs/2403.11914,
ArXiv,Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic,https://arxiv.org/abs/2403.11925,
ArXiv,Explainable Reinforcement Learning-based Home Energy Management Systems using Differentiable Decision Trees,https://arxiv.org/abs/2403.11947,
ArXiv,Supervised Fine-Tuning as Inverse Reinforcement Learning,https://arxiv.org/abs/2403.12017,
ArXiv,Prior-dependent analysis of posterior sampling reinforcement learning with function approximation,https://arxiv.org/abs/2403.11175,
ArXiv,Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data,https://arxiv.org/abs/2403.11841,
ArXiv,Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning,https://arxiv.org/abs/2205.12532,
ArXiv,Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence,https://arxiv.org/abs/2301.11476,
ArXiv,Distributional Reinforcement Learning with Dual Expectile-Quantile Regression,https://arxiv.org/abs/2305.16877,
ArXiv,Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo,https://arxiv.org/abs/2305.18246,
ArXiv,Improving Offline RL by Blending Heuristics,https://arxiv.org/abs/2306.00321,
ArXiv,Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX,https://arxiv.org/abs/2306.09884,
ArXiv,RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment,https://arxiv.org/abs/2307.12950,
ArXiv,MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits,https://arxiv.org/abs/2310.12359,
ArXiv,Understanding when Dynamics-Invariant Data Augmentations Benefit Model-Free Reinforcement Learning Updates,https://arxiv.org/abs/2310.17786,
ArXiv,Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning,https://arxiv.org/abs/2310.18247,
ArXiv,Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization,https://arxiv.org/abs/2311.03351,
ArXiv,Multi-Objective Reinforcement Learning-based Approach for Pressurized Water Reactor Optimization,https://arxiv.org/abs/2312.10194,
ArXiv,BCQQ: Batch-Constraint Quantum Q-Learning with Cyclic Data Re-uploading,https://arxiv.org/abs/2305.00905,
