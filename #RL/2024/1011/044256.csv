feed,title,long_url,short_url
ArXiv,Fostering Intrinsic Motivation in Reinforcement Learning with Pretrained Foundation Models,https://arxiv.org/abs/2410.07404,
ArXiv,CAFEEN: A Cooperative Approach for Energy Efficient NoCs with Multi-Agent Reinforcement Learning,https://arxiv.org/abs/2410.07426,
ArXiv,Zero-Shot Generalization of Vision-Based RL Without Data Augmentation,https://arxiv.org/abs/2410.07441,
ArXiv,Offline Inverse Constrained Reinforcement Learning for Safe-Critical Decision Making in Healthcare,https://arxiv.org/abs/2410.07525,
ArXiv,Parallel Digital Twin-driven Deep Reinforcement Learning for User Association and Load Balancing in Dynamic Wireless Networks,https://arxiv.org/abs/2410.07611,
ArXiv,A Survey for Deep Reinforcement Learning Based Network Intrusion Detection,https://arxiv.org/abs/2410.07612,
ArXiv,StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models,https://arxiv.org/abs/2410.07652,
ArXiv,The Power of Input: Benchmarking Zero-Shot Sim-To-Real Transfer of Reinforcement Learning Control Policies for Quadrotor Control,https://arxiv.org/abs/2410.07686,
ArXiv,StepTool: A Step-grained Reinforcement Learning Framework for Tool Learning in LLMs,https://arxiv.org/abs/2410.07745,
ArXiv,Constrained Skill Discovery: Quadruped Locomotion with Unsupervised Reinforcement Learning,https://arxiv.org/abs/2410.07877,
ArXiv,Meta-Learning Integration in Hierarchical Reinforcement Learning for Advanced Task Complexity,https://arxiv.org/abs/2410.07921,
ArXiv,Efficient Reinforcement Learning with Large Language Model Priors,https://arxiv.org/abs/2410.07927,
ArXiv,Offline Hierarchical Reinforcement Learning via Inverse Optimization,https://arxiv.org/abs/2410.07933,
ArXiv,Neuroplastic Expansion in Deep Reinforcement Learning,https://arxiv.org/abs/2410.07994,
ArXiv,Probabilistic Satisfaction of Temporal Logic Constraints in Reinforcement Learning via Adaptive Policy-Switching,https://arxiv.org/abs/2410.08022,
ArXiv,VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based Verifiers,https://arxiv.org/abs/2410.08048,
ArXiv,Crafting desirable climate trajectories with RL explored socio-environmental simulations,https://arxiv.org/abs/2410.07287,
ArXiv,Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition,https://arxiv.org/abs/2410.07574,
ArXiv,Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery,https://arxiv.org/abs/2410.07643,
ArXiv,Variational Inequality Methods for Multi-Agent Reinforcement Learning: Performance and Stability Gains,https://arxiv.org/abs/2410.07976,
ArXiv,ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages,https://arxiv.org/abs/2306.01460,
