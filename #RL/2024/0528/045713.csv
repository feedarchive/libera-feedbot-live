feed,title,long_url,short_url
ArXiv,Spatio-temporal Value Semantics-based Abstraction for Dense Deep Reinforcement Learning,https://arxiv.org/abs/2405.15829,
ArXiv,Transmission Interface Power Flow Adjustment: A Deep Reinforcement Learning Approach based on Multi-task Attribution Map,https://arxiv.org/abs/2405.15831,
ArXiv,Extended Reality (XR) Codec Adaptation in 5G using Multi-Agent Reinforcement Learning with Attention Action Selection,https://arxiv.org/abs/2405.15872,
ArXiv,Knowledge-Informed Auto-Penetration Testing Based on Reinforcement Learning with Reward Machine,https://arxiv.org/abs/2405.15908,
ArXiv,SF-DQN: Provable Knowledge Transfer using Successor Feature for Deep Reinforcement Learning,https://arxiv.org/abs/2405.15920,
ArXiv,Verified Safe Reinforcement Learning for Neural Network Dynamic Models,https://arxiv.org/abs/2405.15994,
ArXiv,Pausing Policy Learning in Non-stationary Reinforcement Learning,https://arxiv.org/abs/2405.16053,
ArXiv,Finite-Time Analysis for Conflict-Avoidant Multi-Task Reinforcement Learning,https://arxiv.org/abs/2405.16077,
ArXiv,Enabling On-Device Learning via Experience Replay with Efficient Dataset Condensation,https://arxiv.org/abs/2405.16113,
ArXiv,Diffusion-based Reinforcement Learning via Q-weighted Variational Policy Optimization,https://arxiv.org/abs/2405.16173,
ArXiv,Safe Deep Model-Based Reinforcement Learning with Lyapunov Functions,https://arxiv.org/abs/2405.16184,
ArXiv,Adaptive $Q$-Network: On-the-fly Target Selection for Deep Reinforcement Learning,https://arxiv.org/abs/2405.16195,
ArXiv,Deep Reinforcement Learning with Enhanced PPO for Safe Mobile Robot Navigation,https://arxiv.org/abs/2405.16266,
ArXiv,Dynamic Inhomogeneous Quantum Resource Scheduling with Reinforcement Learning,https://arxiv.org/abs/2405.16380,
ArXiv,Safe and Balanced: A Framework for Constrained Multi-Objective Reinforcement Learning,https://arxiv.org/abs/2405.16390,
ArXiv,Reinforcement Learning for Jump-Diffusions,https://arxiv.org/abs/2405.16449,
ArXiv,Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search,https://arxiv.org/abs/2405.16450,
ArXiv,Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control,https://arxiv.org/abs/2405.16485,
ArXiv,Multi-State TD Target for Model-Free Reinforcement Learning,https://arxiv.org/abs/2405.16522,
ArXiv,"An Evolutionary Framework for Connect-4 as Test-Bed for Comparison of Advanced Minimax, Q-Learning and MCTS",https://arxiv.org/abs/2405.16595,
ArXiv,A CMDP-within-online framework for Meta-Safe Reinforcement Learning,https://arxiv.org/abs/2405.16601,
ArXiv,Pick up the PACE: A Parameter-Free Optimizer for Lifelong Reinforcement Learning,https://arxiv.org/abs/2405.16642,
ArXiv,RLSF: Reinforcement Learning via Symbolic Feedback,https://arxiv.org/abs/2405.16661,
ArXiv,Amortized Active Causal Induction with Deep Reinforcement Learning,https://arxiv.org/abs/2405.16718,
ArXiv,Oracle-Efficient Reinforcement Learning for Max Value Ensembles,https://arxiv.org/abs/2405.16739,
ArXiv,Trajectory Data Suffices for Statistically Efficient Learning in Offline RL with Linear $q^\pi$-Realizability and Concentrability,https://arxiv.org/abs/2405.16809,
ArXiv,Knowing What Not to Do: Leverage Language Model Insights for Action Space Pruning in Multi-agent Reinforcement Learning,https://arxiv.org/abs/2405.16854,
ArXiv,Partial Models for Building Adaptive Model-Based Reinforcement Learning Agents,https://arxiv.org/abs/2405.16899,
ArXiv,GTA: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning,https://arxiv.org/abs/2405.16907,
ArXiv,Fast ML-driven Analog Circuit Layout using Reinforcement Learning and Steiner Trees,https://arxiv.org/abs/2405.16951,
ArXiv,Any-step Dynamics Model Improves Future Predictions for Online and Offline Reinforcement Learning,https://arxiv.org/abs/2405.17031,
ArXiv,Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation,https://arxiv.org/abs/2405.17061,
ArXiv,Q-value Regularized Transformer for Offline Reinforcement Learning,https://arxiv.org/abs/2405.17098,
ArXiv,InsigHTable: Insight-driven Hierarchical Table Visualization with Reinforcement Learning,https://arxiv.org/abs/2405.17229,
ArXiv,Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning,https://arxiv.org/abs/2405.17243,
ArXiv,Opinion-Guided Reinforcement Learning,https://arxiv.org/abs/2405.17287,
ArXiv,A Recipe for Unbounded Data Augmentation in Visual Reinforcement Learning,https://arxiv.org/abs/2405.17416,
ArXiv,Biological Neurons Compete with Deep Reinforcement Learning in Sample Efficiency in a Simulated Gameworld,https://arxiv.org/abs/2405.16946,
ArXiv,Analysis of Multiscale Reinforcement Q-Learning Algorithms for Mean Field Control Games,https://arxiv.org/abs/2405.17017,
ArXiv,"A Review of Safe Reinforcement Learning: Methods, Theory and Applications",https://arxiv.org/abs/2205.10330,
ArXiv,Text2Reward: Reward Shaping with Language Models for Reinforcement Learning,https://arxiv.org/abs/2309.11489,
ArXiv,Maximum diffusion reinforcement learning,https://arxiv.org/abs/2309.15293,
ArXiv,Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining,https://arxiv.org/abs/2310.08566,
ArXiv,A Tractable Inference Perspective of Offline RL,https://arxiv.org/abs/2311.00094,
ArXiv,Iterated $Q$-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning,https://arxiv.org/abs/2403.02107,
ArXiv,LTL-Constrained Policy Optimization with Cycle Experience Replay,https://arxiv.org/abs/2404.11578,
ArXiv,Reinforcement Learning Approaches for the Orienteering Problem with Stochastic and Dynamic Release Dates,https://arxiv.org/abs/2207.00885,
ArXiv,Spontaneous Coupling of Q-Learning Algorithms in Equilibrium,https://arxiv.org/abs/2312.02644,
ArXiv,Inverse reinforcement learning by expert imitation for the stochastic linear-quadratic optimal control problem,https://arxiv.org/abs/2405.17085,
