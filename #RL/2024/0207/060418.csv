feed,title,long_url,short_url
ArXiv,Unsupervised Salient Patch Selection for Data-Efficient Reinforcement Learning,https://arxiv.org/abs/2402.03329,
ArXiv,Reinforcement-learning robotic sailboats: simulator and preliminary results,https://arxiv.org/abs/2402.03337,
ArXiv,ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design,https://arxiv.org/abs/2402.03479,
ArXiv,Deep Reinforcement Learning for Picker Routing Problem in Warehousing,https://arxiv.org/abs/2402.03525,
ArXiv,A Reinforcement Learning Approach for Dynamic Rebalancing in Bike-Sharing System,https://arxiv.org/abs/2402.03589,
ArXiv,Assessing the Impact of Distribution Shift on Reinforcement Learning Performance,https://arxiv.org/abs/2402.03590,
ArXiv,Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents,https://arxiv.org/abs/2402.03678,
ArXiv,RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback,https://arxiv.org/abs/2402.03681,
ArXiv,SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems,https://arxiv.org/abs/2402.03741,
ArXiv,Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback,https://arxiv.org/abs/2402.03746,
ArXiv,Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution,https://arxiv.org/abs/2402.03771,
ArXiv,No-Regret Reinforcement Learning in Smooth MDPs,https://arxiv.org/abs/2402.03792,
ArXiv,Compound Returns Reduce Variance in Reinforcement Learning,https://arxiv.org/abs/2402.03903,
ArXiv,Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding,https://arxiv.org/abs/2402.03947,
ArXiv,Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning,https://arxiv.org/abs/2402.03972,
ArXiv,Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning,https://arxiv.org/abs/2402.04080,
ArXiv,Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions,https://arxiv.org/abs/2402.04168,
ArXiv,Reinforcement Learning with Ensemble Model Predictive Safety Certification,https://arxiv.org/abs/2402.04182,
ArXiv,CNN-DRL with Shuffled Features in Finance,https://arxiv.org/abs/2402.03338,
ArXiv,Curriculum reinforcement learning for quantum architecture search under hardware errors,https://arxiv.org/abs/2402.03500,
ArXiv,REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR,https://arxiv.org/abs/2402.03988,
ArXiv,SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning,https://arxiv.org/abs/2402.04114,
ArXiv,Feudal Graph Reinforcement Learning,https://arxiv.org/abs/2304.05099,
ArXiv,XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX,https://arxiv.org/abs/2312.12044,
ArXiv,Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning,https://arxiv.org/abs/2312.15122,
ArXiv,"On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond",https://arxiv.org/abs/2401.03301,
ArXiv,The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise,https://arxiv.org/abs/2401.07844,
ArXiv,Reinforcement Learning Assisted Recursive QAOA,https://arxiv.org/abs/2207.06294,
ArXiv,Collaborative Deep Reinforcement Learning for Resource Optimization in Non-Terrestrial Networks,https://arxiv.org/abs/2402.04056,
