feed,title,long_url,short_url
ArXiv,Surpassing legacy approaches and human intelligence with hybrid single- and multi-objective Reinforcement Learning-based optimization and interpretable AI to enable the economic operation of the US nuclear fleet,https://arxiv.org/abs/2402.11040,
ArXiv,Multi Task Inverse Reinforcement Learning for Common Sense Reward,https://arxiv.org/abs/2402.11367,
ArXiv,Reinforcement learning to maximise wind turbine energy generation,https://arxiv.org/abs/2402.11384,
ArXiv,Federated Reinforcement Learning for Uplink Centric Broadband Communication Optimization over Unlicensed Spectrum,https://arxiv.org/abs/2402.11478,
ArXiv,Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics,https://arxiv.org/abs/2402.11515,
ArXiv,Theoretical foundations for programmatic reinforcement learning,https://arxiv.org/abs/2402.11650,
ArXiv,Combinatorial Client-Master Multiagent Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing,https://arxiv.org/abs/2402.11653,
ArXiv,MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization,https://arxiv.org/abs/2402.11711,
ArXiv,Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation,https://arxiv.org/abs/2402.11760,
ArXiv,Decentralized Multi-Robot Navigation for Autonomous Surface Vehicles with Distributional Reinforcement Learning,https://arxiv.org/abs/2402.11799,
ArXiv,Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization,https://arxiv.org/abs/2402.11835,
ArXiv,Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model,https://arxiv.org/abs/2402.11877,
ArXiv,An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart Target Tracking,https://arxiv.org/abs/2402.12015,
ArXiv,Interpretable Brain-Inspired Representations Improve RL Performance on Visual Navigation Tasks,https://arxiv.org/abs/2402.12067,
ArXiv,Revisiting Data Augmentation in Deep Reinforcement Learning,https://arxiv.org/abs/2402.12181,
ArXiv,CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation,https://arxiv.org/abs/2402.12222,
ArXiv,Non-orthogonal Age-Optimal Information Dissemination in Vehicular Networks: A Meta Multi-Objective Reinforcement Learning Approach,https://arxiv.org/abs/2402.12260,
ArXiv,Cooperative Inverse Reinforcement Learning,https://arxiv.org/abs/1606.03137,
ArXiv,Variance Reduction Based Experience Replay for Policy Optimization,https://arxiv.org/abs/2110.08902,
ArXiv,Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic,https://arxiv.org/abs/2306.02865,
ArXiv,Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning,https://arxiv.org/abs/2307.05209,
ArXiv,Communication-Efficient Decentralized Multi-Agent Reinforcement Learning for Cooperative Adaptive Cruise Control,https://arxiv.org/abs/2308.02345,
ArXiv,On Double Descent in Reinforcement Learning with LSTD and Random Features,https://arxiv.org/abs/2310.05518,
ArXiv,On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning,https://arxiv.org/abs/2310.11840,
ArXiv,Accelerated Policy Gradient: On the Convergence Rates of the Nesterov Momentum for Reinforcement Learning,https://arxiv.org/abs/2310.11897,
ArXiv,Corruption-Robust Offline Reinforcement Learning with General Function Approximation,https://arxiv.org/abs/2310.14550,
ArXiv,DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing,https://arxiv.org/abs/2311.01450,
ArXiv,DRNet: A Decision-Making Method for Autonomous Lane Changingwith Deep Reinforcement Learning,https://arxiv.org/abs/2311.01602,
ArXiv,Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning without Task-Specific Knowledge,https://arxiv.org/abs/2311.09195,
ArXiv,Counting Reward Automata: Sample Efficient Reinforcement Learning Through the Exploitation of Reward Function Structure,https://arxiv.org/abs/2312.11364,
ArXiv,Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving,https://arxiv.org/abs/2401.03160,
ArXiv,Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation,https://arxiv.org/abs/2401.07382,
ArXiv,PRewrite: Prompt Rewriting with Reinforcement Learning,https://arxiv.org/abs/2401.08189,
ArXiv,Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey,https://arxiv.org/abs/2401.11963,
ArXiv,Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools,https://arxiv.org/abs/2311.10801,
ArXiv,Reinforcement Learning for Optimal Execution when Liquidity is Time-Varying,https://arxiv.org/abs/2402.12049,
