feed,title,long_url,short_url
ArXiv,GLIDE-RL: Grounded Language Instruction through DEmonstration in RL,https://arxiv.org/abs/2401.02991v1,
ArXiv,SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning,https://arxiv.org/abs/2401.03137v1,
ArXiv,Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving,https://arxiv.org/abs/2401.03160v1,
ArXiv,An Empirical Investigation of Value-Based Multi-objective Reinforcement Learning for Stochastic Environments,https://arxiv.org/abs/2401.03163v1,
ArXiv,"On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond",https://arxiv.org/abs/2401.03301v1,
ArXiv,MTAC: Hierarchical Reinforcement Learning-based Multi-gait Terrain-adaptive Quadruped Controller,https://arxiv.org/abs/2401.03337v1,
ArXiv,Reinforcement Learning for Distributed Transient Frequency Control with Stability and Safety Guarantees,https://arxiv.org/abs/2207.03329v2,
ArXiv,On the Model-Misspecification in Reinforcement Learning,https://arxiv.org/abs/2306.10694v2,
ArXiv,RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$,https://arxiv.org/abs/2306.15909v3,
ArXiv,Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency,https://arxiv.org/abs/2312.11509v2,
ArXiv,Mutual Information as Intrinsic Reward of Reinforcement Learning Agents for On-demand Ride Pooling,https://arxiv.org/abs/2312.15195v2,
ArXiv,"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents",https://arxiv.org/abs/2401.00812v2,
ArXiv,A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning,https://arxiv.org/abs/2401.02325v2,
