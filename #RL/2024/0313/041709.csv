feed,title,long_url,short_url
ArXiv,Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines,https://arxiv.org/abs/2403.07005,
ArXiv,Sim-to-Real gap in RL: Use Case with TIAGo and Isaac Sim/Gym,https://arxiv.org/abs/2403.07091,
ArXiv,"$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model",https://arxiv.org/abs/2403.07191,
ArXiv,Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control,https://arxiv.org/abs/2403.07216,
ArXiv,Advantage-Aware Policy Optimization for Offline Reinforcement Learning,https://arxiv.org/abs/2403.07262,
ArXiv,Optimization of Pressure Management Strategies for Geological CO2 Sequestration Using Surrogate Model-based Reinforcement Learning,https://arxiv.org/abs/2403.07360,
ArXiv,Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach,https://arxiv.org/abs/2403.07503,
ArXiv,An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning,https://arxiv.org/abs/2403.07566,
ArXiv,Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning,https://arxiv.org/abs/2403.07704,
ArXiv,Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards,https://arxiv.org/abs/2403.07708,
ArXiv,Human-Inspired Framework to Accelerate Reinforcement Learning,https://arxiv.org/abs/2303.08115,
ArXiv,When should we prefer Decision Transformers for Offline Reinforcement Learning?,https://arxiv.org/abs/2305.14550,
ArXiv,Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information,https://arxiv.org/abs/2306.08762,
ArXiv,Enabling self-identification in intelligent agent: insights from computational psychoanalysis,https://arxiv.org/abs/2403.07664,
